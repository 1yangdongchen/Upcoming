These are some basic utility functions we will need.

```python
from __future__ import division
from collections import Counter, defaultdict
import os, math, random, re, glob #Imports a bunch of modules we will need

#
# data splitting
#

def split_data(data, prob):
    """split data into fractions [prob, 1 - prob]"""
    results = [], []
    for row in data:
        results[0 if random.random() < prob else 1].append(row)
    return results

def train_test_split(x, y, test_pct):
    data = zip(x, y)                              # pair corresponding values ; zip combines lists to tuples
    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs
    x_train, y_train = zip(*train)                # magical un-zip trick
    x_test, y_test = zip(*test)
    return x_train, x_test, y_train, y_test
```

# NAIVE BAYES

## Naive Bayes from Scratch

First, let's build a Naive Bayes classifier from scratch. This example drawn
from *Data Science from Scratch* by Joel Grus.

### Mathematical Preliminaries

Recall the key independence assumption of Naive Bayes: $P(X_1 = x_1,\dots,X_n =
x_n\,|\,S) = P(X_1 = x_1\,|\,S)\times \dots
    \times P(X_n = x_n\,|\,S)$

To be concrete, let's assume we are building a spam filter.

Given a vocabulary $w_1,\dots,w_n$, let $X_i$ be the event "message contains
$w_i$." $X_i = x_i, x_i \in \{0,1\}$.

$S$ is the event "message is spam" and $\neg S$ is the event "message is not
spam."

According to Bayes' Theorem

$P(S\,|\,X_1 = x_1,\dots, X_n = x_n) = \frac{P(X_1 = x_1,\dots, X_n =
x_n\,|\,S)P(S)}{P(X_1 = x_1,\dots, X_n = x_n)} = \frac{P(X_1 = x_1,\dots, X_n =
x_n\,|\,S)P(S)}{P(X_1 = x_1,\dots, X_n = x_n\,|\,S)P(S)\, + \,P(X_1 = x_1,\dots,
X_n = x_n\,|\,\neg S)P(\neg S)}$

We further assume that we have no knowledge of the prior probability of spam; so
$P(S) = P(\neg S) = 0.5$ (this is the principle of indifference)

With this simplification, $P(S\,|\,X_1 = x_1,\dots, X_n = x_n) = \frac{P(X_1 =
x_1,\dots, X_n = x_n\,|\,S)}{P(X_1 = x_1,\dots, X_n = x_n\,|\,S)\, +\, P(X_1 =
x_1,\dots, X_n = x_n\,|\,\neg S)}$

Now we make the Naive Bayes assumption: $P(X_1 = x_1,\dots,X_n = x_n\,|\,S) =
P(X_1 = x_1\,|\,S)\times \dots
    \times P(X_n = x_n\,|\,S)$

We can estimate $P(X_i = x_i\,|\,S)$ by computing the fraction of spam messages
containing the word $i$, e.g., Obamacare.

Smoothing: $P(X_i\,|\,S) = \frac{(k + \textrm{number of spams containing}\,
w_i)}{(2k + \textrm{number of spams})}$



### Now we are going to code this up

First we will "tokenize" our input, i.e., turn text into presence/absence of
words.

```python
def tokenize(message): #Note: no stemming
    message = message.lower() # convert to lowercase
    all_words = re.findall("[a-z0-9']+", message) # use re to extract the words
    return set(all_words) # remove duplicates because we are creating a set
```

Now we need to count the number of times each word shows up in spam and non-spam

```python
def count_words(training_set):
    """training set consists of pairs (message, is_spam)""" 
    counts = defaultdict(lambda: [0, 0]) #This is a tricky bit of Python magic, makes a dictionary initialized to [0,0]
    for message, is_spam in training_set: #Here I step through the training set.
        for word in tokenize(message): 
            counts[word][0 if is_spam else 1] += 1
    return counts
```

We need a function to convert these counts into (smoothed) probabilities

```python
def word_probabilities(counts, total_spams, total_non_spams, k=0.5): #What is the smoothing parameter?
    """turn the word_counts into a list of triplets 
    w, p(w | spam) and p(w | ~spam)"""
    return [(w,
             (spam + k) / (total_spams + 2 * k),
             (non_spam + k) / (total_non_spams + 2 * k)) #This uses list comprehension. 
             for w, (spam, non_spam) in counts.items()]  #Replace .iteritems with .items for Python3
```

Now we need to come up with a way to compute the spam probability for a message,
given word probabilities. With the Naive Bayes assumption, we *would* be
multiplying together a bunch of probabilities. This is bad (underflow) so we
compute:

$p_1 *\dots*p_n = \exp(\, \log(p_1) + \dots + \log(p_n)\,)$; recall $\log(ab) =
\log a + \log b$ and $\exp(\, \log x \,) = x$

Thank you, John Napier (1550-1617)

```python
def spam_probability(word_probs, message):
    message_words = tokenize(message)
    log_prob_if_spam = log_prob_if_not_spam = 0.0 #Initialize; we are working with log probs to deal with underflow.

    for word, prob_if_spam, prob_if_not_spam in word_probs: #We iterate over all possible words we've observed

        # for each word in the message, 
        # add the log probability of seeing it 
        if word in message_words:
            log_prob_if_spam += math.log(prob_if_spam) #This is prob of seeing word if spam
            log_prob_if_not_spam += math.log(prob_if_not_spam) #This is prob of seeing word if not spam

        # for each word that's not in the message
        # add the log probability of _not_ seeing it
        else:
            log_prob_if_spam += math.log(1.0 - prob_if_spam)
            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)
            
    prob_if_spam = math.exp(log_prob_if_spam) #Compute numerator
    prob_if_not_spam = math.exp(log_prob_if_not_spam)
    return prob_if_spam / (prob_if_spam + prob_if_not_spam) #Compute whole thing and return
```

Think: how would this change if $P(S) \neq P(\neg S)$

Now we write a class (this is a Python term) for our Naive Bayes Classifier

```python
class NaiveBayesClassifier:

    def __init__(self, k=0.5):
        self.k = k
        self.word_probs = [] #Initializes word_probs as an empty list, sets a default smoothing parameters

    def train(self, training_set): #Operates on the training_set
    
        # count spam and non-spam messages: first step of training
        num_spams = len([is_spam 
                         for message, is_spam in training_set 
                         if is_spam]) #This is also list comprehension
        num_non_spams = len(training_set) - num_spams

        # run training data through our "pipeline"
        word_counts = count_words(training_set)
        self.word_probs = word_probabilities(word_counts, 
                                             num_spams, 
                                             num_non_spams,
                                             self.k) #"Train" classifier 
                                             
    def classify(self, message):
        return spam_probability(self.word_probs, message) #Now we have all we need to classify a message

```

We'll need a special utility function for reading in the data

```python
def get_subject_data(path):

    data = []

    # regex for stripping out the leading "Subject:" and any spaces after it
    subject_regex = re.compile(r"^Subject:\s+")

    # glob.glob returns every filename that matches the wildcarded path
    for fn in glob.glob(path):
        is_spam = "ham" not in fn
        
        with open(fn,'r') as file: #PYTHON 3 USERS: COMMENT THIS OUT AND USE LINE BELOW
        #with open(fn,'r',errors='surrogateescape') as file: 
            for line in file:
                if line.startswith("Subject:"):
                    subject = subject_regex.sub("", line).strip()
                    data.append((subject, is_spam))

    return data
```

Grab data from: https://spamassassin.apache.org/publiccorpus/

Get the ones with 20021010 prefixes and put them into the data folder under the
current working directory.

```python
path = os.getcwd() + '/data/*/*/*'
```

```python
data = get_subject_data(path)
```

```python
len(data) #How many 
```

```python
data[1000] #Let's look
```

To train the model, we'll need to split our data into training & test

```python
random.seed(0) #This is important for replicability
train_data,test_data = split_data(data,0.75) #Recall what the second argument does here
```

```python
classifier = NaiveBayesClassifier() #Create an instance of our classifier
```

```python
classifier.train(train_data) #Train the classifier on the training data
```

```python
len(train_data)
```

Some simple evaluation:

```python
# triplets (subject, actual is_spam, predicted spam probability)
classified = [(subject, is_spam, classifier.classify(subject))
              for subject, is_spam in test_data]

# assume that spam_probability > 0.5 corresponds to spam prediction # and count the combinations of (actual is_spam, predicted is_spam)
counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)
                     for _, is_spam, spam_probability in classified)
```

```python
counts #Let's see how we did!
```

```python
precision = counts[(True,True)]/(counts[(False,True)]+counts[(True,True)]) #True positives over all positive predictions
print(precision)
```

```python
recall = counts[(True,True)]/(counts[(True,False)]+counts[(True,True)])#what fraction of positives identified
print(recall)
```

**Think**: what will happen if I change my spam threshold from 0.5?

**Think**: what decision rule are we using, when we assign the class label
$\hat{y} = S$ when $P(S\,|\,X_1\dots X_n) = \frac{P(S)\prod_i P(X_i =
x_i|S)}{P(\textbf{X})} > 0.5$

**Think**: what will happen if I split the data differently (e.g., less training
data, more testing data). Try it!

We can also find words that lead to a high probability of spam (using Bayes'
Theorem):

```python
def p_spam_given_word(word_prob):
    """uses bayes's theorem to compute p(spam | message contains word)"""
    # word_prob is one of the triplets produced by word_probabilities

    word, prob_if_spam, prob_if_not_spam = word_prob
    return prob_if_spam / (prob_if_spam + prob_if_not_spam)
```

```python
words = sorted(classifier.word_probs,key=p_spam_given_word)
```

```python
spammiest_words = words[-15:]
hammiest_words = words[:15]
```

```python
spammiest_words
```

```python
hammiest_words
```

## Naive Bayes with Scikit-Learn

The examples here are drawn from the *Python Data Science Handbook* by Jake
VanderPlas.

### Gaussian Naive Bayes

This is applied to continuous quantitative features.  The generative model here
assumes that data for each label are drawn from Gaussian distributions. We want
to figure out the parameters of those distributions. If we assume no covariance
between the dimensions, we just find the mean and standard deviation (in each
dimension) of the training data in each label.

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns; sns.set()
```

```python
from sklearn.datasets import make_blobs
X, y = make_blobs(200, 2, centers=2, random_state=2, cluster_std=1.5) #Number of points, number of features (dimensions)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');
#plt.axis([-9,9,-14,4])
```

**Think**: What happens if you change the random state? Centers? Cluster_std?

Note that these really are isotropic... They just are skewed by the plotting.
Try using the commented line
to change axes.

Now let's fit ("learn") the Gaussian Naive Bayes model.

```python
from sklearn.naive_bayes import GaussianNB #Choose class of model
model = GaussianNB() #No hyperparameters
model.fit(X, y) #Fit the model with fit() method
```

To see how this is working, let's see how a bunch of random points are
classified:

```python
rng = np.random.RandomState(0) #Initialize a random number generator
Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2) #These values specially chosen to fill this plane
ynew = model.predict(Xnew)
```

```python
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
lim = plt.axis()
plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.2)
plt.axis(lim);
```

```python
yprob = model.predict_proba(Xnew) #Use this method to examine posterior probabilities of labels
yprob[-8:].round(2)
```

Try varying some of the parameters (e.g., of the blob generator, etc.) and see
what happens.

## Multinomial Naive Bayes

Well suited to text applications, this generating model assumes that the
features are generated by draws from a multinomial distribution (recall this
gives the probability to observe a particular pattern of counts across
features). Features might be, e.g., the count of various words in a text.

```python
from sklearn.datasets import fetch_20newsgroups 
data = fetch_20newsgroups() #Free data to play with: documents from a newsgroup corpus.
data.target_names #Possible categories, i.e., the newsgroups
```

This dataset has a built in breakdown into training and testing sets. We can
pick specific categories, and pull the relevant training and testing sets.

```python
categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] #Can change these of course
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)
```

```python
train['data'][:5]
```

```python
train['target']
```

```python
len(train.data) #See how many training examples
```

```python
len(test.data) #Ditto for testing -- it's about 60% training, 40% testing
```

```python
print(train.data[5]) #Look at an example
#print(train.target_names[5])
```

We need to extract features from the text. We can use built-in feature
extraction to do so. We will use a tf-idf vectorizer, which converts the
document into a vector of words with tf-idf weights (term-frequency inverse-
document frequency). This gives high weight to words that show up a lot in a
given document but rarely across documents in the corpus (more distinctive).

We also take advantage of a cool feature of Scikit-Learn: we can make
pipelines...

```python
from sklearn.feature_extraction.text import TfidfVectorizer  #Feature extraction
from sklearn.naive_bayes import MultinomialNB #Our learner.
from sklearn.pipeline import make_pipeline
model = make_pipeline(TfidfVectorizer(), MultinomialNB()) #This applies the vectorizer, then trains Multinomial NB
```

```python
model.fit(train.data,train.target) #Training syntax: feed the fit method the training data and the training targets
```

```python
labels = model.predict(test.data)
```

```python
labels.size
```

We can even use a confusion matrix!

```python
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(test.target, labels)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
                xticklabels=train.target_names, yticklabels=train.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label');
```

We can also give the model a string and using the predict method see if it can
assign it to a category. This might be the main point of a social science
application.

```python
def predict_category(s, train=train, model=model): #We just define a simple function here
    return train.target_names[model.predict([s])]
```

```python
predict_category('rockets')
```

Try it yourself with your own strings!


**Playtime**: Try altering the categories! See if you can come up with ones that
are more/less likely to be confused. As an extension, you can use different
feature extractors, e.g., CountVectorizer. This just turns each document into a
vector of word counts.

## Let's label these so we can interrogate them

```python
train.target[0:6] #The target vector is just labels, so we need to turn that into real names. 
#Remember that train.target_names turns a label (number) into a real name
```

```python
train.target_names
```

```python
train_target_name_vec = [train.target_names[tarlabel] for tarlabel in train.target] #Get vector of the names
```

```python
train_target_name_vec[0:6]
```

```python
print(train.data[5])
```

Now let's look at the feature vectors for individual documents. As far as I can
tell you need to write a function for this...

```python
vectorizer = TfidfVectorizer() #Need to create an instance of this separately. 
#vectorizer = CountVectorizer()
```

```python
vectorized = vectorizer.fit_transform(train.data) #This creates the feature matrix. 
```

```python
vectorizer.get_feature_names()[4000:4005]
```

```python
vectorizer.vocabulary_.get('space')
```

```python
def feature_vector(vectorized,vectorizer,i): #Need to pass this your trained vectorizer, plus the feature matrix
    feature_name_vec = vectorizer.get_feature_names() #Pull out the vector of feature names from vectorizer
    raw_vec = vectorized.toarray()[i] #Grab the correct row from the feature matrix (turn to array b/c it's sparse)
    feat_vec = [] #This is where we put the results
    for j,line in enumerate(raw_vec): #Iterate through the row of the feature matrix
        if line > 0.: #Most will be zero...
            word = feature_name_vec[j] #If not, pull the actual word
            feat_vec.append([word,line]) #Append the word and the prob
    return feat_vec
    
```

```python
test_vec = feature_vector(vectorized,vectorizer,5)
```

```python
test_vec
```

```python
print(train.data[5])
```

## Now an a more relevant content analysis example

```python
polpath = r"/Users/jevans/Desktop/CAdata/Baseline/*/*" #Wildcards
```

```python
from bunch import *
```

```python
def get_subject_dataP(path):

    datacomplex = Bunch()
    datacomplex['description']='Floor speeches from Congressional establishment Republicans & Tea Partyers'    
    datacomplex['target_names'] = ['establishment', 'teaparty']
    filenames = []
    data = []
    target = []
    
    for fn in glob.glob(path):
        filenames.append(fn)
        file = open(fn,'r')
        data.append(file.read())
        if "clinton" in fn:
            target.append('0')
        elif "lott" in fn:
            target.append('1')
        elif "obama" in fn:
            target.append('2')
        
    datacomplex['filenames']=filenames
    datacomplex['data']=data
    datacomplex['target']=np.array(target)
    
    return datacomplex
```

```python
releases = get_subject_dataP(polpath)
```

```python
len(releases['target'])
```

```python
releases
```

```python
model.fit(releases.data,releases.target)
```

```python
labels = model.predict(releases.data)
```

```python
labels
```

```python
def predict_category(s, train=releases, model=model): #We just define a simple function here
    return train.target_names[model.predict([s])]
```

```python
model.fit_transform
```

```python
predict_category('matplotlib is awesome')
```

# DECISION TREES

We are going to stick to Scikit-Learn here.

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns; sns.set()
```

Decision trees can be used to predict both categorical/class labels (i.e.,
classification) and continuous labels (i.e., regression).

Let's create something to learn:

```python
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=300, centers=4,
                random_state=0, cluster_std=1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');
```

Now we import our Decision Tree classifier from sklearn.tree (very familiar
syntax!) and fit it using the fit method.

```python
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(max_depth=4,random_state=0).fit(X,y)
```

To see what's going on visually with the classification, we can use this
(complex) visualizer.

```python
def visualize_classifier(model, X, y, Xmod, ymod, ax=None, cmap='rainbow'): #X and y are plotted; Xmod and ymod train
    ax = ax or plt.gca()
    # Plot the training points
    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    # fit the estimator
    model.fit(Xmod, ymod)
    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
                         np.linspace(*ylim, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    # Create a color plot with the results
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap=cmap, clim=(y.min(), y.max()),
                           zorder=1)
    ax.set(xlim=xlim, ylim=ylim)
```

Xnew, ynew = make_blobs(n_samples=1000, centers=4,
                random_state=0, cluster_std=1.0)

```python
visualize_classifier(DecisionTreeClassifier(max_depth=1,random_state=0), Xnew, ynew, X, y) #We train on the full data
```

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
accuracy_score(ynew,DecisionTreeClassifier(max_depth=10,random_state=0).fit(X,y).predict(Xnew))
```

```python
depthvec = []
scorevec = []
for i in range(1,20):
    tree2 = DecisionTreeClassifier(max_depth=i,random_state=0).fit(X,y)
    score = accuracy_score(ynew,tree2.predict(Xnew))
    depthvec.append(i)
    scorevec.append(score)
plt.scatter(depthvec,scorevec)
```

Near the cluster boundaries, the shape is pretty weird. Overfitting!

```python
from sklearn.cross_validation import train_test_split #Can use this to split the data
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5,random_state=1) #test_size means a 50/50 split
print(len(X))
print(len(Xtrain))
print(len(Xtest))
```

With some abuse of notation, we can use the visualizer to plot first a
classifier trained on the "training" half of the data.

```python
visualize_classifier(DecisionTreeClassifier(max_depth=5,random_state=0), X, y, Xtrain, ytrain) #Train with half the data
```

Now we can train on the second half of the data--allegedly the test data--to see
how different training sets affect the decision boundaries.

```python
visualize_classifier(DecisionTreeClassifier(random_state=0), X, y, Xtest, ytest) #Train with other half of the data
```

Finally, we can get a sense of the performance by training on the training data,
but PLOTTING the test data

```python
visualize_classifier(DecisionTreeClassifier(), Xtest, ytest, Xtrain, ytrain) #Trains with train data, plots test data
```

Combining multiple overfitting estimators turns out to be a key idea in machine
learning. This is called **bagging** and is a type of **ensemble** method. The
idea is to make many randomized estimators--each can overfit, as decision trees
are wont to do--but then to combine them, ultimately producing a better
classification. A **random forest** is produced by bagging decision trees.

```python
from sklearn.tree import DecisionTreeClassifier  #Just in case
from sklearn.ensemble import BaggingClassifier #The bagging

tree = DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.

bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data
```

```python
?BaggingClassifier #Learn more
```

```python
bag.fit(X,y) #Fit the bagged classifier
```

```python
Xnew, ynew = make_blobs(n_samples=10000, centers=4,
                random_state=0, cluster_std=1.0)
```

```python
visualize_classifier(bag,Xnew,ynew,X,y) #And visualize
#Remember we can give the full data as training data, as bag automatically splits and trains
```

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
recall_score(ynew,bag.predict(Xnew),average='weighted')
```

## Brief aside: Random forest regression

First, let's create a challenging dataset.

```python
rng = np.random.RandomState(42)
x = 10 * rng.rand(200) #200 uniformly distributed random numbers between 0 and 10
def model(x, sigma=0.3): 
    fast_oscillation = np.sin(5 * x) 
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * rng.randn(len(x)) #Create 200 random numbers, normally distributed

    return slow_oscillation + fast_oscillation + noise
```

```python
y = model(x)
plt.errorbar(x, y, 0.3, fmt='o'); #Shows one std around point
#plt.scatter(x, y); #Plots the actual points
```

First we will try to learn this with something simple; a single decision tree,
but one that is a regressor, not a classifier.

```python
#First try to learn this with something simple --  a single decision tree regressor
from sklearn.tree import DecisionTreeRegressor

# Fit regression models
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_3 = DecisionTreeRegressor(max_depth=10)
regr_1.fit(x[:,None], y)
regr_2.fit(x[:,None], y)
regr_3.fit(x[:,None], y)

# Predict
xfit = np.linspace(0, 10, 1000)
yfit_1 = regr_1.predict(xfit[:, None]) #xfit[:,None] is a work-around to pass a 1d feature matrix
yfit_2 = regr_2.predict(xfit[:, None])
yfit_3 = regr_3.predict(xfit[:, None])
```

Let's see how these two decision trees do... What do you think will happen as we
add depth?

```python
plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit_1, '-r'); #depth = 2
plt.plot(xfit,yfit_2,'-b'); #depth = 5
plt.plot(xfit,yfit_3,'-g'); #depth = 10
```

Or we can use a **random forest regressor**

```python
from sklearn.ensemble import RandomForestRegressor 
forest = RandomForestRegressor(200) 
forest.fit(x[:, None], y)
    
yfit = forest.predict(xfit[:, None]) #Predictions of the forest model
ytrue = model(xfit, sigma=0) #This is the underlying data, no noise
```

```python
plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit, '-r');
plt.plot(xfit, ytrue, '-k', alpha=0.5);
```

A beautiful fit of the underlying pattern... a lot of the noise has been washed
out. This is the power of bagging.

# K-MEANS

Remember the basic goal of k-means: we want to partition our input data into a
set of $k$ clusters, $\textbf{S} = \{S_1,\dots,S_k\}$ such that we minimize the
sum of squared distances from each point in a cluster to the center of its
assigned cluster, i.e., $\arg\min_S \sum_{i=1}^{k}\sum_{\textbf{x} \in S_i} \|
\textbf{x} - \mathbf{\mu}_i \|^{2}$

We can approximately solve this using a very simple E-M (Expectation-
Maximization) algorithm. The basic idea is:

1. Pick the number of clusters

2. Randomly choose initial cluster centers

3. Repeat until converged
    1. assign points to nearest cluster center (E-step)
    2. set the cluster centers to the mean (M-step)

Here is one nearly "from scratch" implementation, from *The Python Data Science
Handbook*

```python
from sklearn.metrics import pairwise_distances_argmin #One little thing we'll import instead of writing up

def find_clusters(X, n_clusters, rseed=2): #STEP 1: Pick the number of clusters here; set the hyperparameter
    #STEP 2: Randomly choose initial cluster centers
    rng = np.random.RandomState(rseed)
    i = rng.permutation(X.shape[0])[:n_clusters] #X.shape[0] gives you the number of rows in the data, i.e. #points
    centers = X[i]
    
    while True: #STEP 3: Repeat until converged...
        # 3A. Assign labels based on closest center (E-step)
        labels = pairwise_distances_argmin(X, centers)
    
        # 3B. Find new centers from means of points (M-step)
        new_centers = np.array([X[labels == i].mean(0)
                                for i in range(n_clusters)]) #This goes through all clusters and computes means.
        # 3C. Check for convergence
        if np.all(centers == new_centers): 
            break #Stop if converged
        centers = new_centers #otherwise use the new centers
    
    return centers, labels
```

Let's test it out!

```python
from sklearn.datasets.samples_generator import make_blobs 
X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)
plt.scatter(X[:, 0], X[:, 1], s=50);
```

```python
centers, labels = find_clusters(X, 4)
plt.scatter(X[:, 0], X[:, 1], c=labels,
            s=50, cmap='rainbow');
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
```

Of course there is a way to do this in Scikit-Learn:

```python
from sklearn.cluster import KMeans 
kmeans = KMeans(n_clusters=4) 
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
```

Let's plot this!

```python
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='rainbow')
centers2 = kmeans.cluster_centers_
plt.scatter(centers2[:, 0],centers2[:, 1], c='black', s=200, alpha=0.5);
```

But there are some issues with E-M.

Issue 1) You may not find the global optimum, because this is a heuristic
algorithm!

```python
centers, labels = find_clusters(X, 4, rseed=0) #Set different seed, so it picks different starting points
plt.scatter(X[:, 0], X[:, 1], c=labels,
            s=50, cmap='rainbow');
```

Ugh! So you usually want to run this a few times.

Issue 2. You have to select the number of clusters beforehand (i.e., it has a
hyperparameter). K-means doesn't know how to learn the number of clusters from
the data. For example...

```python
labels = KMeans(8, random_state=0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
            s=50, cmap='rainbow');
```

There are many ways to solve this; one of the simplest is called silhouette
analysis.

```python
#See: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html 
from __future__ import print_function

from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

# Generating the sample data from make_blobs
# This particular setting has one distict cluster and 3 clusters placed close
# together.
X, y = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)
range_n_clusters = [2, 3, 4, 5, 6]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhoutte score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors)

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1],
                marker='o', c="white", alpha=1, s=200)

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()
```

Issue 3. It is also limited to linear cluster boundaries (this has to do with
the cluster criterion). But we can easily find clusters that don't have linear
boundaries...

```python
from sklearn.datasets import make_moons
X, y = make_moons(200, noise=.05, random_state=0)
labels = KMeans(2, random_state=0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
            s=50, cmap='rainbow');
```

Remember that we often want to put data in a lower dimension; but sometimes
putting data into a higher dimension can help. Using a kernel transformation we
represent the data in higher dimensions, then use k-means. Scikit-Learn has a
way to do this using Spectral Clustering.

```python
from sklearn.cluster import SpectralClustering
model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',
                           assign_labels='kmeans')
labels = model.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
            s=50, cmap='rainbow');
```

Can you do silhouette analysis on spectral clustering? I think yes?

Issue 4. k-means can be slow on big datasets (because we have to compute all
those cluster distances and cluster averages at each step). There are ways to
speed it up, e.g., by recomputing the centers from a random subset. This is
called MiniBatch k-means.

A fancier relative of k-means is the Gaussian Mixture Model. We will probably
discuss that tomorrow.

```python
#See: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html 
from __future__ import print_function

from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

# Generating the sample data from make_blobs
# This particular setting has one distict cluster and 3 clusters placed close
# together.
X, y = make_moons(200, noise=.05, random_state=0)
labels = KMeans(2, random_state=0).fit_predict(X)

range_n_clusters = [2, 3, 4, 5, 6]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',
                           assign_labels='kmeans',random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhoutte score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors)

    # Labeling the clusters
    #centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    #ax2.scatter(centers[:, 0], centers[:, 1],
               # marker='o', c="white", alpha=1, s=200)

    #for i, c in enumerate(centers):
        #ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()
```

```python

```
