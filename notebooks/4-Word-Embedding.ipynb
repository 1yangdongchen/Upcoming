{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout\n",
    "\n",
    "+ Intro algebra\n",
    "    + news\n",
    "    + physics\n",
    "+ doc2vec\n",
    "    + press releases\n",
    "+ score function\n",
    "    + gensim\n",
    "+ dimensions example, e.g. he-she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import random\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "intro stuff ...\n",
    "\n",
    "# Getting our corpuses\n",
    "\n",
    "Instead of downloading our corpora, we have download them ahead of time, a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `data/grimmerPressReleases`. So we will load them into a DataFrame, to do this first we need to define a function to convert directories of text files into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDir(targetDir, category):\n",
    "    allFileNames = os.listdir(targetDir)\n",
    "    #We need to make them into useable paths and filter out hidden files\n",
    "    filePaths = [os.path.join(targetDir, fname) for fname in allFileNames if fname[0] != '.']\n",
    "\n",
    "    #The dict that will become the DataFrame\n",
    "    senDict = {\n",
    "        'category' : [category] * len(filePaths),\n",
    "        'filePath' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "\n",
    "    for fPath in filePaths:\n",
    "        with open(fPath) as f:\n",
    "            senDict['text'].append(f.read())\n",
    "            senDict['filePath'].append(fPath)\n",
    "\n",
    "    return pandas.DataFrame(senDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function in all the directories in `data/grimmerPressReleases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filePath</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Apr2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Dec2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Feb2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE      Fact sheet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Feb2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Jun2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  BOSTON  MA  Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Mar2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01May2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  The President ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\01Nov2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  Washington  DC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\02Aug2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases\\Kennedy\\02Feb2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     The Preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           filePath  \\\n",
       "0   Kennedy  data/grimmerPressReleases\\Kennedy\\01Apr2005Ken...   \n",
       "10  Kennedy  data/grimmerPressReleases\\Kennedy\\01Dec2005Ken...   \n",
       "20  Kennedy  data/grimmerPressReleases\\Kennedy\\01Feb2006Ken...   \n",
       "30  Kennedy  data/grimmerPressReleases\\Kennedy\\01Feb2007Ken...   \n",
       "40  Kennedy  data/grimmerPressReleases\\Kennedy\\01Jun2007Ken...   \n",
       "50  Kennedy  data/grimmerPressReleases\\Kennedy\\01Mar2007Ken...   \n",
       "60  Kennedy  data/grimmerPressReleases\\Kennedy\\01May2007Ken...   \n",
       "70  Kennedy  data/grimmerPressReleases\\Kennedy\\01Nov2007Ken...   \n",
       "80  Kennedy  data/grimmerPressReleases\\Kennedy\\02Aug2006Ken...   \n",
       "90  Kennedy  data/grimmerPressReleases\\Kennedy\\02Feb2005Ken...   \n",
       "\n",
       "                                                 text  \n",
       "0            FOR IMMEDIATE RELEASE   FOR IMMEDIATE...  \n",
       "10           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "20           FOR IMMEDIATE RELEASE      Fact sheet...  \n",
       "30           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "40           FOR IMMEDIATE RELEASE  BOSTON  MA  Se...  \n",
       "50           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "60           FOR IMMEDIATE RELEASE  The President ...  \n",
       "70           FOR IMMEDIATE RELEASE  Washington  DC...  \n",
       "80           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...  \n",
       "90           FOR IMMEDIATE RELEASE     The Preside...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = 'data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pandas.DataFrame()\n",
    "\n",
    "for senatorName in [d for d in os.listdir(dataDir) if d[0] != '.']:\n",
    "    senPath = os.path.join(dataDir, senatorName)\n",
    "    senReleasesDF = senReleasesDF.append(loadDir(senPath, senatorName), ignore_index = True)\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming is taking a really long time do to the size of the dataset, so it's been disabled for now\n",
    "\n",
    "We also want to remove stop words and stem, but tokenizing requires two steps. Word2Vec wants to know the sentence structure as well as simply the words, so the tokenizing is slightly different this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the same function as last week\n",
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w).encode('utf8') for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "#initialize our stemmer and our stop words\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [normlizeTokens(s, stopwordLst = stop_words_nltk, stemmer = None) for s in x])\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
    "\n",
    "To load our data our data we give all the sentences to the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('administration', 0.7934945225715637),\n",
       " ('presidents', 0.7433978319168091),\n",
       " ('administrations', 0.6810588836669922),\n",
       " ('george', 0.6101688146591187),\n",
       " ('cheney', 0.5860358476638794),\n",
       " ('ronald', 0.5486534237861633),\n",
       " ('responds', 0.5284579992294312),\n",
       " ('republican', 0.5097452998161316),\n",
       " ('rollback', 0.5044845342636108),\n",
       " ('reject', 0.5019153356552124)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.98172605e-01,   4.08544064e-01,  -2.00023651e-02,\n",
       "         6.83706284e-01,   2.35091284e-01,   1.51089147e-01,\n",
       "        -3.50266784e-01,  -1.78749275e+00,  -5.86134017e-01,\n",
       "         3.16251256e-02,   9.05968964e-01,   5.65631092e-01,\n",
       "        -6.18162632e-01,   1.28233993e+00,  -8.36357772e-01,\n",
       "        -9.27805007e-01,   2.24624681e+00,   1.43553746e+00,\n",
       "        -4.64852333e-01,  -8.20181429e-01,   1.01894200e+00,\n",
       "        -3.12417209e-01,  -1.17840540e+00,   8.21972936e-02,\n",
       "        -4.99140956e-02,   1.39575958e+00,   1.46494284e-01,\n",
       "         1.27444255e+00,  -1.05740400e-02,  -1.64721191e+00,\n",
       "        -3.50008100e-01,  -1.23521760e-01,  -1.50235698e-01,\n",
       "         1.78061342e+00,  -7.88266510e-02,   5.62229812e-01,\n",
       "        -7.34569281e-02,  -3.37678730e-01,  -1.54030669e+00,\n",
       "         1.08246878e-01,  -7.28343070e-01,  -4.37717676e-01,\n",
       "        -9.53186810e-01,   9.64138806e-01,  -7.94973791e-01,\n",
       "         1.58517861e+00,  -1.26691723e+00,  -5.57923675e-01,\n",
       "         3.41233611e-02,  -7.89384723e-01,   1.79420173e+00,\n",
       "        -3.25704664e-01,   1.72457248e-01,   2.99194809e-02,\n",
       "        -2.25236535e-01,   1.32633179e-01,   1.03272593e+00,\n",
       "        -1.29017234e-01,   1.11102951e+00,   1.39927793e+00,\n",
       "        -9.27605689e-01,   1.10574317e+00,  -1.21472681e+00,\n",
       "        -3.73198614e-02,  -3.51771235e-01,   5.38281262e-01,\n",
       "         1.04273379e+00,   2.05210641e-01,   6.93444550e-01,\n",
       "         3.50998163e-01,   5.51785886e-01,   1.24009228e+00,\n",
       "         2.04215690e-01,   3.00711781e-01,   1.60589755e+00,\n",
       "         1.09868526e+00,  -4.61392194e-01,  -3.33091943e-04,\n",
       "        -6.58234954e-01,   1.17781949e+00,   8.26978147e-01,\n",
       "         6.47615492e-01,   9.99076605e-01,   7.57358611e-01,\n",
       "         1.19216907e+00,  -1.71377342e-02,  -1.03397334e+00,\n",
       "         2.34086245e-01,   1.62554586e+00,   1.11160636e+00,\n",
       "         1.40415120e+00,   1.12538600e+00,   9.16893482e-01,\n",
       "         3.45241159e-01,  -3.17260355e-01,  -1.22201830e-01,\n",
       "        -2.04447672e-01,  -3.41390818e-01,   3.42949569e-01,\n",
       "         2.54303277e-01], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V['president']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09916329,  0.37337583, -0.30811408, ...,  2.64301944,\n",
       "        -0.14985117, -0.47056434],\n",
       "       [ 1.59650826,  1.67542541, -1.05237496, ...,  2.99215102,\n",
       "        -1.2940805 ,  0.85386258],\n",
       "       [ 0.59042859,  0.29354835, -0.11482511, ...,  0.39653182,\n",
       "         0.65785706, -0.88093102],\n",
       "       ..., \n",
       "       [ 0.05379788,  0.07380242, -0.03090563, ...,  0.03676279,\n",
       "        -0.02846254, -0.01079885],\n",
       "       [-0.03350566,  0.01471397,  0.02765373, ..., -0.07439245,\n",
       "        -0.02612949, -0.00890458],\n",
       "       [ 0.05079404,  0.04173233, -0.06164182, ...,  0.0689274 ,\n",
       "         0.01979096,  0.04595068]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find what doesn't fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.doesnt_match(['she', 'he', 'her', 'him', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or save for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"data/senpressreleasesWORD2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APS abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsDF = pandas.read_csv('data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF['tokenized_sents'] = apsDF['abstract'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "apsDF['normalized_sents'] = apsDF['tokenized_sents'].apply(lambda x: [normlizeTokens(s, stopwordLst = stop_words_nltk, lemmer = wordnet) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's change some parameters. Let's use skipgrams instead of CBOW (continous bag of words), have a 200-dimensional vector space, only keep words that appear more than 2 times, and iterate the training algorithm over the corpus 10 times. For more information about parametrizing word2vec models, please see [here](https://radimrehurek.com/gensim/models/word2vec.html) and [here](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsW2V = gensim.models.word2vec.Word2Vec(apsDF['normalized_sents'].sum(), sg = 1, size = 200, min_count= 2, iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsW2V.save('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then also load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsW2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some vector algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guaranteeing', 0.7645862102508545),\n",
       " ('papapetrou', 0.7472379207611084),\n",
       " ('biquadratic', 0.7393147945404053),\n",
       " ('speck', 0.7356243133544922),\n",
       " ('admitting', 0.734847903251648)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apsW2V.most_similar(positive = ['newton', 'relativity'], negative = ['einstein'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News from The New York Times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "f = open('data/nytimes_full.txt', 'r')\n",
    "for row in f:\n",
    "    if row != '\\n' and  row != \"';\\n\":\n",
    "        paragraphs.append(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(paragraphs)), 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = [paragraphs[i] for i in sorted(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
