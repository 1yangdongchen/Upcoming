{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some basic utility functions we will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "import os, math, random, re, glob #Imports a bunch of modules we will need\n",
    "import pandas, requests, json\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions for splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y)                              # pair corresponding values ; zip combines lists to tuples\n",
    "    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs\n",
    "    x_train, y_train = zip(*train)                # magical un-zip trick\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes from Scratch\n",
    "\n",
    "First, let's build a Naive Bayes classifier from scratch. This example drawn from *Data Science from Scratch* by Joel Grus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall the key independence assumption of Naive Bayes: $P(X_1 = x_1,\\dots,X_n = x_n\\,|\\,S) = P(X_1 = x_1\\,|\\,S)\\times \\dots \n",
    "    \\times P(X_n = x_n\\,|\\,S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be concrete, let's assume we are building a spam filter. \n",
    "\n",
    "Given a vocabulary $w_1,\\dots,w_n$, let $X_i$ be the event \"message contains $w_i$.\" $X_i = x_i, x_i \\in \\{0,1\\}$. \n",
    "\n",
    "$S$ is the event \"message is spam\" and $\\neg S$ is the event \"message is not spam.\"\n",
    "\n",
    "According to Bayes' Theorem \n",
    "\n",
    "$P(S\\,|\\,X_1 = x_1,\\dots, X_n = x_n) = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)}{P(X_1 = x_1,\\dots, X_n = x_n)} = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)}{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)P(S)\\, + \\,P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,\\neg S)P(\\neg S)}$\n",
    "\n",
    "We further assume that we have no knowledge of the prior probability of spam; so $P(S) = P(\\neg S) = 0.5$ (this is the principle of indifference)\n",
    "\n",
    "With this simplification, $P(S\\,|\\,X_1 = x_1,\\dots, X_n = x_n) = \\frac{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)}{P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,S)\\, +\\, P(X_1 = x_1,\\dots, X_n = x_n\\,|\\,\\neg S)}$ \n",
    "\n",
    "Now we make the Naive Bayes assumption: $P(X_1 = x_1,\\dots,X_n = x_n\\,|\\,S) = P(X_1 = x_1\\,|\\,S)\\times \\dots \n",
    "    \\times P(X_n = x_n\\,|\\,S)$\n",
    "    \n",
    "We can estimate $P(X_i = x_i\\,|\\,S)$ by computing the fraction of spam messages containing the word $i$, e.g., Obamacare. \n",
    "\n",
    "Smoothing: $P(X_i\\,|\\,S) = \\frac{(k + \\textrm{number of spams containing}\\, w_i)}{(2k + \\textrm{number of spams})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to code this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will \"tokenize\" our input, i.e., turn text into presence/absence of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message): #Note: no stemming\n",
    "    message = message.lower() # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message) # use re to extract the words\n",
    "    return set(all_words) # remove duplicates because we are creating a set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to count the number of times each word shows up in spam and non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\" \n",
    "    counts = defaultdict(lambda: [0, 0]) #This is a tricky bit of Python magic, makes a dictionary initialized to [0,0]\n",
    "    for message, is_spam in training_set: #Here I step through the training set.\n",
    "        for word in tokenize(message): \n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to convert these counts into (smoothed) probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5): #What is the smoothing parameter?\n",
    "    \"\"\"turn the word_counts into a list of triplets \n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k)) #This uses list comprehension. \n",
    "             for w, (spam, non_spam) in counts.items()]  #Replace .iteritems with .items for Python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to come up with a way to compute the spam probability for a message, given word probabilities. With the Naive Bayes assumption, we *would* be multiplying together a bunch of probabilities. This is bad (underflow) so we compute:\n",
    "\n",
    "$p_1 *\\dots*p_n = \\exp(\\, \\log(p_1) + \\dots + \\log(p_n)\\,)$; recall $\\log(ab) = \\log a + \\log b$ and $\\exp(\\, \\log x \\,) = x$\n",
    "\n",
    "Thank you, John Napier (1550-1617)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0 #Initialize; we are working with log probs to deal with underflow.\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs: #We iterate over all possible words we've observed\n",
    "\n",
    "        # for each word in the message, \n",
    "        # add the log probability of seeing it \n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam) #This is prob of seeing word if spam\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam) #This is prob of seeing word if not spam\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam) #Compute numerator\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam) #Compute whole thing and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think: how would this change if $P(S) \\neq P(\\neg S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a class (this is a Python term) for our Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = [] #Initializes word_probs as an empty list, sets a default smoothing parameters\n",
    "\n",
    "    def train(self, training_set): #Operates on the training_set\n",
    "    \n",
    "        # count spam and non-spam messages: first step of training\n",
    "        num_spams = len([is_spam \n",
    "                         for message, is_spam in training_set \n",
    "                         if is_spam]) #This is also list comprehension\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, \n",
    "                                             num_spams, \n",
    "                                             num_non_spams,\n",
    "                                             self.k) #\"Train\" classifier \n",
    "                                             \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message) #Now we have all we need to classify a message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a special utility function for reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "        \n",
    "        #with open(fn,'r') as file: #PYTHON 3 USERS: COMMENT THIS OUT AND USE LINE BELOW\n",
    "        with open(fn,'r',errors='surrogateescape') as file: \n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab data from: https://spamassassin.apache.org/publiccorpus/\n",
    "\n",
    "Get the ones with 20021010 prefixes and put them into the data folder under the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data/*/*/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_subject_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3423"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #How many "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Re: The 3rd Annual Consult Hyperion Digital Identity Forum', False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000] #Let's look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we'll need to split our data into training & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0) #This is important for replicability\n",
    "train_data,test_data = split_data(data,0.75) #Recall what the second argument does here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier() #Create an instance of our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.train(train_data) #Train the classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2547"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# triplets (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction # and count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)\n",
    "                     for _, is_spam, spam_probability in classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, False): 704,\n",
       "         (False, True): 33,\n",
       "         (True, False): 38,\n",
       "         (True, True): 101})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts #Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753731343283582\n"
     ]
    }
   ],
   "source": [
    "precision = counts[(True,True)]/(counts[(False,True)]+counts[(True,True)]) #True positives over all positive predictions\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266187050359713\n"
     ]
    }
   ],
   "source": [
    "recall = counts[(True,True)]/(counts[(True,False)]+counts[(True,True)])#what fraction of positives identified\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think**: what will happen if I change my spam threshold from 0.5?\n",
    "\n",
    "**Think**: what decision rule are we using, when we assign the class label $\\hat{y} = S$ when $P(S\\,|\\,X_1\\dots X_n) = \\frac{P(S)\\prod_i P(X_i = x_i|S)}{P(\\textbf{X})} > 0.5$\n",
    "\n",
    "**Think**: what will happen if I split the data differently (e.g., less training data, more testing data). Try it!\n",
    "\n",
    "We can also find words that lead to a high probability of spam (using Bayes' Theorem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"uses bayes's theorem to compute p(spam | message contains word)\"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sorted(classifier.word_probs,key=p_spam_given_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spammiest_words = words[-15:]\n",
    "hammiest_words = words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('account', 0.01780821917808219, 0.00022893772893772894),\n",
       " ('norton', 0.02054794520547945, 0.00022893772893772894),\n",
       " ('per', 0.02054794520547945, 0.00022893772893772894),\n",
       " ('29', 0.02054794520547945, 0.00022893772893772894),\n",
       " ('95', 0.02054794520547945, 0.00022893772893772894),\n",
       " ('mortgage', 0.023287671232876714, 0.00022893772893772894),\n",
       " ('guaranteed', 0.023287671232876714, 0.00022893772893772894),\n",
       " ('adv', 0.026027397260273973, 0.00022893772893772894),\n",
       " ('zzzz', 0.026027397260273973, 0.00022893772893772894),\n",
       " ('clearance', 0.026027397260273973, 0.00022893772893772894),\n",
       " ('year', 0.028767123287671233, 0.00022893772893772894),\n",
       " ('sale', 0.031506849315068496, 0.00022893772893772894),\n",
       " ('rates', 0.031506849315068496, 0.00022893772893772894),\n",
       " ('systemworks', 0.036986301369863014, 0.00022893772893772894),\n",
       " ('money', 0.03972602739726028, 0.00022893772893772894)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammiest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spambayes', 0.0013698630136986301, 0.04601648351648352),\n",
       " ('users', 0.0013698630136986301, 0.036401098901098904),\n",
       " ('razor', 0.0013698630136986301, 0.030906593406593408),\n",
       " ('zzzzteana', 0.0013698630136986301, 0.029075091575091576),\n",
       " ('sadev', 0.0013698630136986301, 0.026785714285714284),\n",
       " ('ouch', 0.0013698630136986301, 0.02358058608058608),\n",
       " ('bliss', 0.0013698630136986301, 0.021749084249084248),\n",
       " ('selling', 0.0013698630136986301, 0.021291208791208792),\n",
       " ('apt', 0.0013698630136986301, 0.021291208791208792),\n",
       " ('wedded', 0.0013698630136986301, 0.021291208791208792),\n",
       " ('perl', 0.0013698630136986301, 0.0190018315018315),\n",
       " ('spamassassin', 0.0013698630136986301, 0.018543956043956044),\n",
       " ('satalk', 0.00410958904109589, 0.053800366300366304),\n",
       " ('problem', 0.0013698630136986301, 0.017170329670329672),\n",
       " ('alsa', 0.0013698630136986301, 0.016712454212454212)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammiest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let' Try it on Our Clinton Obama Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Obama's data\n",
      "Fetching Clinton's data\n"
     ]
    }
   ],
   "source": [
    "def getGithubFiles(target, maxFiles = 100):\n",
    "    #We are setting a max so our examples don't take too long to run\n",
    "    #For converting to a DataFrame\n",
    "    releasesDict = {\n",
    "        'name' : [], #The name of the file\n",
    "        'text' : [], #The text of the file, watch out for binary files\n",
    "        'path' : [], #The path in the git repo to the file\n",
    "        'html_url' : [], #The url to see the file on Github\n",
    "        'download_url' : [], #The url to download the file\n",
    "    }\n",
    "\n",
    "    #Get the directory information from Github\n",
    "    r = requests.get(target)\n",
    "\n",
    "    #Check for rate limiting\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(\"Github didn't like your request, you have probably been rate limited.\")\n",
    "    filesLst = json.loads(r.text)\n",
    "\n",
    "    for fileDict in filesLst[:maxFiles]:\n",
    "        #These are provided by the directory\n",
    "        releasesDict['name'].append(fileDict['name'])\n",
    "        releasesDict['path'].append(fileDict['path'])\n",
    "        releasesDict['html_url'].append(fileDict['html_url'])\n",
    "        releasesDict['download_url'].append(fileDict['download_url'])\n",
    "\n",
    "        #We need to download the text though\n",
    "        text = requests.get(fileDict['download_url']).text\n",
    "        releasesDict['text'].append(text)\n",
    "\n",
    "    return pandas.DataFrame(releasesDict)\n",
    "\n",
    "'''\n",
    "#Uncomment this to download your own data\n",
    "\n",
    "targetSenator = 'Obama'# = ['Voinovich', 'Obama', 'Whitehouse', 'Snowe', 'Rockefeller', 'Murkowski', 'McCain', 'Kyl', 'Baucus', 'Frist']\n",
    "\n",
    "ObamaReleases = pandas.DataFrame()\n",
    "\n",
    "print(\"Fetching {}'s data\".format(targetSenator))\n",
    "targetDF = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/{}'.format(targetSenator), maxFiles = 2000)\n",
    "targetDF['targetSenator'] = targetSenator\n",
    "ObamaReleases = ObamaReleases.append(targetDF, ignore_index = True)\n",
    "\n",
    "targetSenator = 'Clinton'# = ['Voinovich', 'Obama', 'Whitehouse', 'Snowe', 'Rockefeller', 'Murkowski', 'McCain', 'Kyl', 'Baucus', 'Frist']\n",
    "\n",
    "ClintonReleases = pandas.DataFrame()\n",
    "\n",
    "print(\"Fetching {}'s data\".format(targetSenator))\n",
    "targetDF = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/{}'.format(targetSenator), maxFiles = 2000)\n",
    "targetDF['targetSenator'] = targetSenator\n",
    "ClintonReleases = ClintonReleases.append(targetDF, ignore_index = True)\n",
    "\n",
    "ObamaClintonReleases = ObamaReleases.append(ClintonReleases, ignore_index = True)\n",
    "ObamaClintonReleases.to_csv(\"data/ObamaClintonReleases.csv\")\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#senReleasesTraining = pandas.read_csv(\"data/senReleasesTraining.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ObamaClintonReleases = pandas.read_csv(\"data/ObamaClintonReleases.csv\")\n",
    "ObamaClintonReleases = ObamaClintonReleases.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the 'targetSenator' column into a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IsObama(targetSenator):\n",
    "    if targetSenator == 'Obama':\n",
    "        isObama = True\n",
    "    elif targetSenator == 'Clinton':\n",
    "        isObama = False\n",
    "    return(isObama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ObamaClintonReleases['IsObama'] = ObamaClintonReleases['targetSenator'].apply(lambda x: IsObama(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(zip(ObamaClintonReleases['text'], ObamaClintonReleases['IsObama']))\n",
    "random.seed(0) #This is important for replicability\n",
    "train_data,test_data = split_data(data,0.75) #Recall what the second argument does here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n",
      "434\n"
     ]
    }
   ],
   "source": [
    "print (len(train_data))\n",
    "print (len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier() \n",
    "classifier.train(train_data) #Train the classifier on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple evaluation:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
