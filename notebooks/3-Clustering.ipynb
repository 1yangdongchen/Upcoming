{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout\n",
    "\n",
    "+ Opening\n",
    "+ packages\n",
    "    + scikit-learn\n",
    "    + gensim\n",
    "+ Introduce ML\n",
    "+ Training\n",
    "    + Use sklearn dataset examples\n",
    "+ Extraction\n",
    "    + Probabilistic models\n",
    "    + Deterministic models\n",
    "    + stop lists\n",
    "    + cleaning\n",
    "    + Performance\n",
    "    + [sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)\n",
    "        + [Tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "        + [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "        + Hierarchical\n",
    "    + [gensim](http://radimrehurek.com/gensim/apiref.html)\n",
    "        + [LDA](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "            + Expansions on LDA\n",
    "        + [word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "        + Doc2Vec\n",
    "    + Implement our own models\n",
    "        + regressions\n",
    "\n",
    "# Week 3 - Clustering\n",
    "\n",
    "Intro stuff ...\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import metaknowledge as mk\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a dataset to work on from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "#data_home argument will let you change the download location\n",
    "\n",
    "newsgroups = sklearn.datasets.fetch_20newsgroups(subset='train')\n",
    "print(dir(newsgroups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the categories with `target_names` or the actual files with `filenames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)\n",
    "print(len(newsgroups.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets reduce our dataset for this analysis and drop some of the extraneous information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos']\n",
    "newsgroups = sklearn.datasets.fetch_20newsgroups(subset='train', categories = categories, remove=['headers', 'footers', 'quotes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents are stored in `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350\n",
      "Looking for a VIDEO in and OUT Video card for the IBM.  One that will\n",
      "allow you to watch TV (coax) or video IN, and will do Video out,\n",
      "digitize pictures.  and if I am in Windows, and would like to be able to\n",
      "look the RCA out for the card to my TV and have it display on there, as\n",
      "well as DOS apps.\n",
      "\n",
      "I heard of these SNES and Genesis copiers, that will copy any games, are\n",
      "those for real?\n",
      "                                                                                                                            \n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups.data))\n",
    "print(\"\\n\".join(newsgroups.data[2].split(\"\\n\")[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 23525)\n",
      "3121\n"
     ]
    }
   ],
   "source": [
    "count_vect = sklearn.feature_extraction.text.CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(newsgroups.data)\n",
    "print(X_train_counts.shape)\n",
    "print(count_vect.vocabulary_.get('algorithm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 23525)\n"
     ]
    }
   ],
   "source": [
    "tf_transformer = sklearn.feature_extraction.text.TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 23525)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('liabilities', 0.21420446505887239),\n",
       " ('swimwear', 0.13747920830953553),\n",
       " ('mercs', 0.088365351304721923),\n",
       " ('intrinsicsp', 0.091564463286724435),\n",
       " ('tia', 0.041290324202552547),\n",
       " ('tackling', 0.06544181933636592),\n",
       " ('0040000d', 0.1414485456965873),\n",
       " ('260', 0.16071862845194229),\n",
       " ('wpd', 0.096273767261683074),\n",
       " ('sabotage', 0.072904541887069643),\n",
       " ('bailey', 0.1850416732187698),\n",
       " ('diaphram', 0.12929531261824911),\n",
       " ('rpk105', 0.087311129052314446),\n",
       " ('suit', 0.21969075879490346),\n",
       " ('magnus', 0.12823336632667126)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(count_vect.vocabulary_.keys(), X_train_tfidf.data))[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of garabge from unique words and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('george', 0.076706593286425179),\n",
       " ('german', 0.02664763900591155),\n",
       " ('germany', 0.14498027575724973),\n",
       " ('gets', 0.032641029874755159),\n",
       " ('getting', 0.036245068939312432),\n",
       " ('gf', 0.028235479836039856),\n",
       " ('gfx', 0.029258280771752555),\n",
       " ('gfxbase', 0.034749257578655317),\n",
       " ('ghg', 0.022975631270420991),\n",
       " ('ghost', 0.032641029874755159)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, max_features=10000, min_df=3, stop_words='english', norm='l2', use_idf=True)\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "list(zip(vectorizer.get_feature_names()[3000:3010], X.data[3000:3010]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = np.unique(newsgroups.target_names).shape[0]\n",
    "true_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=4, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=1)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 4397.963\n",
      "Iteration  1, inertia 2243.047\n",
      "Iteration  2, inertia 2239.600\n",
      "Iteration  3, inertia 2236.259\n",
      "Iteration  4, inertia 2233.050\n",
      "Iteration  5, inertia 2230.648\n",
      "Iteration  6, inertia 2229.491\n",
      "Iteration  7, inertia 2228.807\n",
      "Iteration  8, inertia 2228.342\n",
      "Iteration  9, inertia 2228.034\n",
      "Iteration 10, inertia 2227.781\n",
      "Iteration 11, inertia 2227.598\n",
      "Iteration 12, inertia 2227.443\n",
      "Iteration 13, inertia 2227.328\n",
      "Iteration 14, inertia 2227.270\n",
      "Iteration 15, inertia 2227.232\n",
      "Iteration 16, inertia 2227.218\n",
      "Iteration 17, inertia 2227.201\n",
      "Iteration 18, inertia 2227.151\n",
      "Iteration 19, inertia 2227.125\n",
      "Iteration 20, inertia 2227.105\n",
      "Iteration 21, inertia 2227.088\n",
      "Iteration 22, inertia 2227.076\n",
      "Iteration 23, inertia 2227.070\n",
      "Converged at iteration 23: center shift 0.000000e+00 within tolerance 1.325517e-08\n",
      "done in 1.135s\n",
      "Homogeneity: 0.410\n",
      "Completeness: 0.463\n",
      "V-measure: 0.435\n",
      "Adjusted Rand-Index: 0.308\n",
      "Silhouette Coefficient: -0.001\n"
     ]
    }
   ],
   "source": [
    "km = sklearn.cluster.KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=1)\n",
    "\n",
    "print(\"Clustering sparse data with {}\".format(km))\n",
    "t0 = time.time()\n",
    "km.fit(X) #km.fit(X_train_tfidf)\n",
    "print(\"done in {:0.3f}s\".format(time.time() - t0))\n",
    "\n",
    "print(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(newsgroups.target, km.labels_)))\n",
    "print(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(newsgroups.target, km.labels_)))\n",
    "print(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(newsgroups.target, km.labels_)))\n",
    "print(\"Adjusted Rand-Index: {:.3f}\".format(sklearn.metrics.adjusted_rand_score(newsgroups.target, km.labels_)))\n",
    "print(\"Silhouette Coefficient: {:0.3f}\".format(sklearn.metrics.silhouette_score(X, newsgroups.target, sample_size=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.homogeneity_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " 00\n",
      " sale\n",
      " offer\n",
      " shipping\n",
      " condition\n",
      " new\n",
      " asking\n",
      " interested\n",
      " sell\n",
      " email\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      " car\n",
      " like\n",
      " just\n",
      " don\n",
      " new\n",
      " good\n",
      " cars\n",
      " think\n",
      " know\n",
      " use\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      " mac\n",
      " apple\n",
      " drive\n",
      " thanks\n",
      " know\n",
      " does\n",
      " monitor\n",
      " card\n",
      " simms\n",
      " use\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      " window\n",
      " server\n",
      " motif\n",
      " application\n",
      " program\n",
      " widget\n",
      " use\n",
      " using\n",
      " x11r5\n",
      " file\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "loading abstracts from raw wos data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bibliometric approach is explored to trackin...</td>\n",
       "      <td>[Moed, Henk F., Halevi, Gali]</td>\n",
       "      <td>WOS:000345136000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper compared and contrasted patent coun...</td>\n",
       "      <td>[Sung, Hui-Yun, Wang, Chun-Chieh, Chen, Dar-Ze...</td>\n",
       "      <td>WOS:000339379600015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Triple Helix (TH) network of bi- and trilate...</td>\n",
       "      <td>[Ivanova, Inga A., Leydesdorff, Loet]</td>\n",
       "      <td>WOS:000335905000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bibliometric analysis was conducted to evalu...</td>\n",
       "      <td>[Tan, Jiang, Fu, Hui-Zhen, Ho, Yuh-Shan]</td>\n",
       "      <td>WOS:000330622600043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nowadays, the development of emerging technolo...</td>\n",
       "      <td>[Wang, Xuefeng, Li, Rongrong, Ren, Shiming, Zh...</td>\n",
       "      <td>WOS:000331559800010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is examined whether the number (J) of (join...</td>\n",
       "      <td>[Bougrine, Hassan]</td>\n",
       "      <td>WOS:000330622600016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scientific co-authorship of African researcher...</td>\n",
       "      <td>[Pouris, Anastassios, Ho, Yuh-Shan]</td>\n",
       "      <td>WOS:000331559800037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In this article barycenters of the places of p...</td>\n",
       "      <td>[Verleysen, Frederik T., Engels, Tim C. E.]</td>\n",
       "      <td>WOS:000343609900032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>While there is a large body of research analyz...</td>\n",
       "      <td>[Yoshikane, Fuyuki, Suzuki, Takafumi]</td>\n",
       "      <td>WOS:000331559800019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This paper analyzes the relationship among res...</td>\n",
       "      <td>[Ibanez, Alfonso, Bielza, Concha, Larranaga, P...</td>\n",
       "      <td>WOS:000317746900012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  A bibliometric approach is explored to trackin...   \n",
       "1  This paper compared and contrasted patent coun...   \n",
       "2  A Triple Helix (TH) network of bi- and trilate...   \n",
       "3  A bibliometric analysis was conducted to evalu...   \n",
       "4  Nowadays, the development of emerging technolo...   \n",
       "5  It is examined whether the number (J) of (join...   \n",
       "6  Scientific co-authorship of African researcher...   \n",
       "7  In this article barycenters of the places of p...   \n",
       "8  While there is a large body of research analyz...   \n",
       "9  This paper analyzes the relationship among res...   \n",
       "\n",
       "                                             authors                   id  \n",
       "0                      [Moed, Henk F., Halevi, Gali]  WOS:000345136000022  \n",
       "1  [Sung, Hui-Yun, Wang, Chun-Chieh, Chen, Dar-Ze...  WOS:000339379600015  \n",
       "2              [Ivanova, Inga A., Leydesdorff, Loet]  WOS:000335905000018  \n",
       "3           [Tan, Jiang, Fu, Hui-Zhen, Ho, Yuh-Shan]  WOS:000330622600043  \n",
       "4  [Wang, Xuefeng, Li, Rongrong, Ren, Shiming, Zh...  WOS:000331559800010  \n",
       "5                                 [Bougrine, Hassan]  WOS:000330622600016  \n",
       "6                [Pouris, Anastassios, Ho, Yuh-Shan]  WOS:000331559800037  \n",
       "7        [Verleysen, Frederik T., Engels, Tim C. E.]  WOS:000343609900032  \n",
       "8              [Yoshikane, Fuyuki, Suzuki, Takafumi]  WOS:000331559800019  \n",
       "9  [Ibanez, Alfonso, Bielza, Concha, Larranaga, P...  WOS:000317746900012  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RC = mk.RecordCollection('../data/imetricrecs.txt')\n",
    "recsData = {'abstract' : [], 'id' : [], 'authors' : []}\n",
    "for R in RC:\n",
    "    if R.get('abstract') is not None:\n",
    "        recsData['abstract'].append(R.get('abstract'))\n",
    "        recsData['id'].append(R.id)\n",
    "        recsData['authors'].append(R.get('authorsFull'))\n",
    "imetric_abstracts = pd.DataFrame(recsData)\n",
    "imetric_abstracts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tokenize and filter the abstracts a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>id</th>\n",
       "      <th>abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bibliometric approach is explored to trackin...</td>\n",
       "      <td>[Moed, Henk F., Halevi, Gali]</td>\n",
       "      <td>WOS:000345136000022</td>\n",
       "      <td>[bibliometric, approach, is, explored, trackin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper compared and contrasted patent coun...</td>\n",
       "      <td>[Sung, Hui-Yun, Wang, Chun-Chieh, Chen, Dar-Ze...</td>\n",
       "      <td>WOS:000339379600015</td>\n",
       "      <td>[this, paper, compared, contrasted, patent, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Triple Helix (TH) network of bi- and trilate...</td>\n",
       "      <td>[Ivanova, Inga A., Leydesdorff, Loet]</td>\n",
       "      <td>WOS:000335905000018</td>\n",
       "      <td>[triple, helix, (th), network, bi-, trilateral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bibliometric analysis was conducted to evalu...</td>\n",
       "      <td>[Tan, Jiang, Fu, Hui-Zhen, Ho, Yuh-Shan]</td>\n",
       "      <td>WOS:000330622600043</td>\n",
       "      <td>[bibliometric, analysis, was, conducted, evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nowadays, the development of emerging technolo...</td>\n",
       "      <td>[Wang, Xuefeng, Li, Rongrong, Ren, Shiming, Zh...</td>\n",
       "      <td>WOS:000331559800010</td>\n",
       "      <td>[nowadays,, development, emerging, technology,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is examined whether the number (J) of (join...</td>\n",
       "      <td>[Bougrine, Hassan]</td>\n",
       "      <td>WOS:000330622600016</td>\n",
       "      <td>[it, is, examined, whether, number, (j), (join...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scientific co-authorship of African researcher...</td>\n",
       "      <td>[Pouris, Anastassios, Ho, Yuh-Shan]</td>\n",
       "      <td>WOS:000331559800037</td>\n",
       "      <td>[scientific, co-authorship, african, researche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In this article barycenters of the places of p...</td>\n",
       "      <td>[Verleysen, Frederik T., Engels, Tim C. E.]</td>\n",
       "      <td>WOS:000343609900032</td>\n",
       "      <td>[this, article, barycenters, places, publicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>While there is a large body of research analyz...</td>\n",
       "      <td>[Yoshikane, Fuyuki, Suzuki, Takafumi]</td>\n",
       "      <td>WOS:000331559800019</td>\n",
       "      <td>[while, there, is, large, body, research, anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This paper analyzes the relationship among res...</td>\n",
       "      <td>[Ibanez, Alfonso, Bielza, Concha, Larranaga, P...</td>\n",
       "      <td>WOS:000317746900012</td>\n",
       "      <td>[this, paper, analyzes, relationship, among, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  A bibliometric approach is explored to trackin...   \n",
       "1  This paper compared and contrasted patent coun...   \n",
       "2  A Triple Helix (TH) network of bi- and trilate...   \n",
       "3  A bibliometric analysis was conducted to evalu...   \n",
       "4  Nowadays, the development of emerging technolo...   \n",
       "5  It is examined whether the number (J) of (join...   \n",
       "6  Scientific co-authorship of African researcher...   \n",
       "7  In this article barycenters of the places of p...   \n",
       "8  While there is a large body of research analyz...   \n",
       "9  This paper analyzes the relationship among res...   \n",
       "\n",
       "                                             authors                   id  \\\n",
       "0                      [Moed, Henk F., Halevi, Gali]  WOS:000345136000022   \n",
       "1  [Sung, Hui-Yun, Wang, Chun-Chieh, Chen, Dar-Ze...  WOS:000339379600015   \n",
       "2              [Ivanova, Inga A., Leydesdorff, Loet]  WOS:000335905000018   \n",
       "3           [Tan, Jiang, Fu, Hui-Zhen, Ho, Yuh-Shan]  WOS:000330622600043   \n",
       "4  [Wang, Xuefeng, Li, Rongrong, Ren, Shiming, Zh...  WOS:000331559800010   \n",
       "5                                 [Bougrine, Hassan]  WOS:000330622600016   \n",
       "6                [Pouris, Anastassios, Ho, Yuh-Shan]  WOS:000331559800037   \n",
       "7        [Verleysen, Frederik T., Engels, Tim C. E.]  WOS:000343609900032   \n",
       "8              [Yoshikane, Fuyuki, Suzuki, Takafumi]  WOS:000331559800019   \n",
       "9  [Ibanez, Alfonso, Bielza, Concha, Larranaga, P...  WOS:000317746900012   \n",
       "\n",
       "                                                 abs  \n",
       "0  [bibliometric, approach, is, explored, trackin...  \n",
       "1  [this, paper, compared, contrasted, patent, co...  \n",
       "2  [triple, helix, (th), network, bi-, trilateral...  \n",
       "3  [bibliometric, analysis, was, conducted, evalu...  \n",
       "4  [nowadays,, development, emerging, technology,...  \n",
       "5  [it, is, examined, whether, number, (j), (join...  \n",
       "6  [scientific, co-authorship, african, researche...  \n",
       "7  [this, article, barycenters, places, publicati...  \n",
       "8  [while, there, is, large, body, research, anal...  \n",
       "9  [this, paper, analyzes, relationship, among, r...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split())\n",
    "def abstractFilter(abString):\n",
    "    sents = nltk.sent_tokenize(abString)\n",
    "    texts = [word for sent in sents for word in sent.lower().split() if word not in stoplist]\n",
    "    return texts\n",
    "\n",
    "imetric_abstracts['abs'] = imetric_abstracts['abstract'].apply(abstractFilter)\n",
    "imetric_abstracts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(imetric_abstracts['abs'])\n",
    "bigrammed = (bigram[imetric_abstracts['abs']])\n",
    "trigram = gensim.models.Phrases(bigrammed)\n",
    "trigrammed = (trigram[bigrammed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.405695915222168\n"
     ]
    }
   ],
   "source": [
    "modelSaveLoc = '../imetricsmodel'\n",
    "\n",
    "start = time.time()\n",
    "model = gensim.models.Word2Vec(trigrammed, workers=4, batch_words=10000)\n",
    "\n",
    "for iteration in range(10):\n",
    "    model.train(trigrammed)\n",
    "\n",
    "vocab_matrix = model.syn0\n",
    "vocabulary = model.index2word\n",
    "\n",
    "model.save(modelSaveLoc)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3949624 , -0.22495362,  0.15555388,  0.43530717,  0.69420147,\n",
       "       -0.32774708, -0.18479134,  0.39014307, -0.45391074, -0.05440942,\n",
       "        0.10095476,  0.27137527, -0.0231375 ,  0.34073639,  0.09912231,\n",
       "       -0.18628053, -0.21471679,  0.01861936, -0.08713119,  0.23915564,\n",
       "        0.04913566,  0.31107786,  0.29575941, -0.49446738, -0.14355353,\n",
       "       -0.03819824, -0.01955821, -0.38507015,  0.01835928,  0.23673563,\n",
       "        0.19441639,  0.1636242 , -0.09832141,  0.50012702, -0.19998699,\n",
       "        0.11287316, -0.12851557, -0.13711846,  0.26076308, -0.30599201,\n",
       "        0.44110763,  0.38901994,  0.13803509,  0.26594684, -0.01821186,\n",
       "       -0.13151573, -0.02712949, -0.25130689, -0.30537039,  0.08836263,\n",
       "        0.44266155,  0.0689806 ,  0.22709419, -0.39330652, -0.16798811,\n",
       "        0.54647827,  0.22472142, -0.08905712, -0.15847112,  0.40746388,\n",
       "        0.18080944, -0.1040563 ,  0.12385511,  0.60678685,  0.44112048,\n",
       "       -0.72353733, -0.13739991, -0.4349502 , -0.31172612,  0.13975222,\n",
       "       -0.00431089, -0.03848824, -0.18319598, -0.17596933, -0.46157101,\n",
       "       -0.33628583,  0.47919354, -0.34396461, -0.07311814,  0.49192238,\n",
       "       -0.54527044,  0.22306056, -0.67802852,  0.19698352,  0.68411833,\n",
       "       -0.52541476,  0.34197918, -0.51571709, -0.17237416,  0.21147071,\n",
       "       -0.53888464, -0.35848734, -0.1891534 , -0.11452752, -0.05179487,\n",
       "        0.05132388, -0.0529165 , -0.25272802, -0.19806902, -0.26902121], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(modelSaveLoc)\n",
    "model['renewable']"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
