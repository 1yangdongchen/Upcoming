{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, FAST_VERSION, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set this to wherever you put your files, if not in the same directory as the .ipynb file\n",
    "wd = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### each line of the following file is treated as a \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'APSabstracts1950s.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-174e7dc99b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsvReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'APSabstracts1950s.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'APSabstracts1950s.csv'"
     ]
    }
   ],
   "source": [
    "csvReader = csv.reader(open('APSabstracts1950s.csv', 'r'))\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in csvReader:\n",
    "    sents = nltk.sent_tokenize(row[3])\n",
    "    doc = []\n",
    "    #lowers all cases; removes words in stoplist\n",
    "    texts = [[word for word in sent.lower().split() if word not in stoplist]for sent in sents]\n",
    "    for text in texts:\n",
    "        doc.extend(text)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes words that appear less than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        frequency[token] +=1\n",
    "docs = [[token for token in doc if frequency[token] > 1]for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(docs)\n",
    "bigrammed = (bigram[docs])\n",
    "trigram = gensim.models.Phrases(bigrammed)\n",
    "trigrammed = (trigram[bigrammed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710.480762959\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = gensim.models.Word2Vec(trigrammed, workers=4, batch_words=10000)\n",
    "\n",
    "for iteration in range(10):\n",
    "    model.train(trigrammed)\n",
    "\n",
    "vocab_matrix = model.syn0\n",
    "vocabulary = model.index2word\n",
    "\n",
    "model.save('APS1950model2')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('APS1950model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11366612,  0.6733855 ,  0.29729691,  0.34947798,  2.24851561,\n",
       "       -0.57647777,  0.20599896,  0.8607595 , -0.5167051 ,  0.53721344,\n",
       "       -0.0368433 , -1.206236  ,  0.88823003,  2.4509449 ,  0.60654026,\n",
       "        0.25592163, -0.51826233,  0.51866895, -0.7907387 , -1.2144618 ,\n",
       "        1.17498791, -1.45251107, -0.81177002, -0.16462365, -1.34767425,\n",
       "       -0.28736192,  0.92019171, -1.79114592,  0.72128701,  0.55047262,\n",
       "       -0.33360377,  0.02686848, -0.61845279,  1.08052635,  0.27933291,\n",
       "        0.53220147, -0.631818  , -0.7925266 ,  1.39817464, -0.50639665,\n",
       "       -0.31766775,  1.92057657,  2.07173252,  1.28246355,  2.03123116,\n",
       "        0.56315064,  0.14823908,  0.70236444, -1.37610137,  0.8148486 ,\n",
       "        0.94313467, -0.30077243, -0.23178385, -0.23589949,  0.02888884,\n",
       "        1.83044732, -1.36353433,  0.16784276, -0.60782135,  0.02768349,\n",
       "       -1.77468729,  1.86033702, -0.38472942, -0.25378212,  0.25974628,\n",
       "       -0.74237055, -0.11020777, -0.65182447,  2.33068514,  2.85477996,\n",
       "       -0.38830569,  0.47405204,  0.25324079, -1.01544607, -0.81558031,\n",
       "        0.24399708, -1.63007116, -0.64813727,  0.8471452 , -2.03254986,\n",
       "        0.43974426,  1.25022817,  0.99866986, -0.5013904 ,  0.12804142,\n",
       "        1.6280694 ,  3.02537346, -1.22500038, -0.44657847,  1.36468613,\n",
       "        0.99385875,  0.20354898, -0.1633794 ,  1.61782825, -1.17337191,\n",
       "       -1.05004144, -0.73892713,  0.13873476, -0.75564259,  1.27161181], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['matter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28767101051060878"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('gas', 'solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'organic', 0.44844162464141846),\n",
       " (u'weak-field', 0.43419504165649414),\n",
       " (u'room', 0.42750734090805054),\n",
       " (u'solids', 0.41425949335098267),\n",
       " (u'moderately', 0.3968007564544678),\n",
       " (u'liquid', 0.39475369453430176),\n",
       " (u'analyticity', 0.3718644380569458),\n",
       " (u'aqueous', 0.3556833863258362),\n",
       " (u'degeneracy,', 0.34781524538993835),\n",
       " (u'-mesonic_atoms', 0.3470304012298584)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['solid', 'liquids'], negative=['gas'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now documents from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvReader = csv.reader(open('/clintonAPSabstracts1950s.csv', 'r'))\n",
    "docs = []\n",
    "for row in csvReader:\n",
    "    sents = nltk.sent_tokenize(row[3])\n",
    "    doc = []\n",
    "    #lowers all cases; removes words in stoplist\n",
    "    texts = [[word for word in sent.lower().split() if word not in stoplist]for sent in sents]\n",
    "    for text in texts:\n",
    "        doc.extend(text)\n",
    "    docs.append(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/home/jevans/clinton'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "tokens = wordlist.words('2009-Obama.txt')\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Doc2Vec documentation: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    Doc2Vec blog post about installing a fast C compiler to make this soooooo much faster: http://rare-technologies.com/word2vec-in-python-part-two-optimizing/\n",
    "        - This was a headache for me a bit, so lemme know if theres problems.\n",
    "        - With these toy examples the speed is not a factor however, so it probably can be disregarded.\n",
    "'''\n",
    "patt = r'[,\\.-_]+$'\n",
    "\n",
    "#This is to make sure you have the proper underlying Cython stuff worked up\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_docs = [\"Hey there, this is a test of the national broadcast system.\",\n",
    "         \"Dogs are very cute animals.\",\n",
    "         \"Didn't you get a super cute pet the other day?\",\n",
    "         \"This is a nightmare, the world is on fire.\",\n",
    "         \"How many times do I have to tell you?\"]\n",
    "\n",
    "test_docs = [\"My bird makes for a great pet.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_docs = []\n",
    "ctr = 0\n",
    "for doc in input_docs:\n",
    "    labeled_docs.append(TaggedDocument(words=[re.sub(patt,'', x) for x in doc.lower().split()], tags=['%s'%(str(ctr))])) #Add these TaggedDocuments to a list, which are the datastructure the model takes\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(labeled_docs, size=100, window=10, min_count=1, workers=4) #Plenty more parameters to mess with. \n",
    "\n",
    "for epoch in xrange(0,10):\n",
    "    random.shuffle(labeled_docs)\n",
    "    model.train(labeled_docs, total_examples=len(labeled_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.668450117229\n",
      "-0.0349147375627\n"
     ]
    }
   ],
   "source": [
    "print model.similarity('cute', 'pet')\n",
    "print model.similarity('cute', 'fire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See which sentences in the input docs are closest to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: My bird makes for a great pet.\n",
      "#0) Didn't you get a super cute pet the other day?\n",
      "\tdistance:0.548879\n",
      "#1) How many times do I have to tell you?\n",
      "\tdistance:0.548928\n",
      "#2) This is a nightmare, the world is on fire.\n",
      "\tdistance:1.143749\n"
     ]
    }
   ],
   "source": [
    "docs_mat = np.zeros((len(model.docvecs), 100))\n",
    "\n",
    "for ix_ in xrange(len(model.docvecs)):\n",
    "    vec = model.docvecs[ix_]\n",
    "    docs_mat[ix_, :] = vec\n",
    "    \n",
    "for doc in test_docs:\n",
    "    vec = model.infer_vector([re.sub(patt, '', x) for x in doc.lower().split()])\n",
    "    res = cdist(np.reshape(vec, (1,vec.size)), model.docvecs, 'cosine') #distance between doc & every doc in the input_docs\n",
    "    sorted_ix = np.argsort(res[0])\n",
    "\n",
    "    sorted_res = res[0, sorted_ix]\n",
    "    print \"query: %s\" % doc\n",
    "    for i in range(3):\n",
    "        print \"#%d) %s\" %(i, input_docs[sorted_ix[i]])\n",
    "        print \"\\tdistance:%f\" %(sorted_res[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LDA Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a corpus. In this example, the file should be a line-delimeted set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are better (in terms of RAM) ways to load the corpus such that is encoded as it's read.\n",
    "documents = []\n",
    "fin       = open(wd + \"/tasa.docs-line-delimited-20k\", \"rb\")\n",
    "for line in fin: documents.append(line.rstrip(\"\\n\"))\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: utterly important...can take a while (~1 minute on my desktop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) load and remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = set()\n",
    "fin = open(wd + \"/stopwords.txt\",\"rb\") # A Gerow special\n",
    "for w in fin: stoplist.add(w.rstrip(\"\\n\"))\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the lowercasing...\n",
    "texts = [[word.lower() for word in document.lower().split() if word not in stoplist] for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Remove words that appear only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens  = sum(texts, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts       = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "## Other important things people tend to do here, in order of importance:\n",
    "##    1) Deal with punctuation.\n",
    "##    2) Remove words with a low and high document frequency.\n",
    "##    3) Remove words below a minimum average TF*IDF value.\n",
    "##    4) Stem words (NLTK has a couple good stemmers).\n",
    "##    5) Remove words not found in a dictionary (I have a great / huge dictionary; can take a while).\n",
    "##    6) Compute and include bigrams that have a collocational strength above some threshold (I use a top n sort of thing).\n",
    "##       scikit-learn has a great, but convoluted collocation extraction implementation.\n",
    "\n",
    "## The best rule of thumb I've heard for 2-4 is that the resulting dictionary should be between 15k and 25k words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary (also called the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert our corpus to the gensim sparse representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the great part of the gensim implementations\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Don't need to run this for this example####\n",
    "\n",
    "## Getting to this point may take a while on large corpora. Luckily, you can serialize\n",
    "##   the corpus in the sparse representation for future use:\n",
    "\n",
    "corpora.MmCorpus.serialize(wd + '/tasa.mm', corpus) # store to disk, for later use\n",
    "dictionary.save(wd + '/tasa.dict')\n",
    "\n",
    "## And read it back in:\n",
    "corpus     = corpora.MmCorpus(wd + '/tasa.mm')\n",
    "dictionary = corpora.Dictionary.load(wd + '/tasa.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now some topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here, I'm using Hierarchical Dirichlet Processe (HDP)\n",
    "##   which is just LDA that fits its own parameter for the number of topics. Gensim has\n",
    "##   a handful of other models, but HDP is one of the more advanced.\n",
    "## You may get some warnings for the first few iterations here.\n",
    "hdpmodel = models.HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the topics (a bit ugly, but you get the picture...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'topic 0: 0.003*it + 0.003*is + 0.002*he + 0.002*that + 0.002*was + 0.002*you + 0.002*as + 0.002*be + 0.002*but + 0.001*at + 0.001*on + 0.001*an + 0.001*or + 0.001*could + 0.001*his',\n",
       " u'topic 1: 0.003*is + 0.003*that + 0.002*are + 0.002*have + 0.002*you + 0.002*it + 0.001*or + 0.001*i + 0.001*their + 0.001*he + 0.001*but + 0.001*not + 0.001*this + 0.001*was + 0.001*be',\n",
       " u'topic 2: 0.003*is + 0.002*that + 0.002*you + 0.001*it + 0.001*are + 0.001*as + 0.001*be + 0.001*from + 0.001*have + 0.001*he + 0.001*they + 0.001*or + 0.001*not + 0.001*people + 0.001*was',\n",
       " u'topic 3: 0.002*is + 0.002*that + 0.001*socrates\" + 0.001*you + 0.001*it + 0.001*be + 0.001*not + 0.001*are + 0.001*out + 0.001*was + 0.001*she + 0.001*has + 0.001*on + 0.001*less + 0.001*milk',\n",
       " u'topic 4: 0.002*is + 0.001*on + 0.001*that + 0.001*you + 0.001*it + 0.001*world, + 0.001*himself + 0.001*can + 0.001*he + 0.001*was + 0.001*are + 0.001*their + 0.001*gertrude + 0.001*cabin. + 0.001*gain',\n",
       " u'topic 5: 0.002*is + 0.001*are + 0.001*on + 0.001*it + 0.001*that + 0.001*was + 0.001*his + 0.001*at + 0.001*beam + 0.001*from + 0.001*anger, + 0.001*hitler\"s + 0.001*most + 0.001*paintings + 0.001*poisoning',\n",
       " u'topic 6: 0.002*is + 0.002*that + 0.001*were + 0.001*it + 0.001*you + 0.001*as + 0.001*people + 0.001*on + 0.001*they + 0.001*was + 0.001*but + 0.001*are + 0.001*not + 0.001*he + 0.001*school.\"',\n",
       " u'topic 7: 0.002*is + 0.001*that + 0.001*are + 0.001*it + 0.001*you + 0.001*aggressive + 0.001*they + 0.001*africa. + 0.001*pittsburgh + 0.001*cabinet + 0.001*israel\"s + 0.001*was + 0.001*we + 0.001*soft, + 0.001*be',\n",
       " u'topic 8: 0.002*is + 0.001*it + 0.001*he + 0.001*are + 0.001*that + 0.001*one + 0.001*you + 0.001*she + 0.001*have + 0.001*unusually + 0.001*at + 0.001*sometimes + 0.001*i + 0.001*was + 0.001*people.\"',\n",
       " u'topic 9: 0.001*is + 0.001*was + 0.001*are + 0.001*gallon + 0.001*that + 0.001*stirring + 0.001*baby,\" + 0.001*on + 0.001*tail + 0.001*you + 0.001*these + 0.001*grace + 0.001*borders + 0.001*it + 0.001*monsieur',\n",
       " u'topic 10: 0.001*is + 0.001*was + 0.001*drugs, + 0.001*their + 0.001*as + 0.001*she + 0.001*sportif + 0.001*are + 0.001*made + 0.001*that + 0.001*not + 0.001*you + 0.001*he + 0.001*generating + 0.001*interest,',\n",
       " u'topic 11: 0.002*is + 0.001*they + 0.001*that + 0.001*you + 0.001*it + 0.001*are + 0.001*have + 0.001*dogs. + 0.001*\"hi, + 0.001*with + 0.001*agreed + 0.001*from + 0.001*so + 0.001*on + 0.001*as',\n",
       " u'topic 12: 0.002*is + 0.001*that + 0.001*but + 0.001*they + 0.001*from + 0.001*as + 0.001*my + 0.001*price + 0.001*down, + 0.001*are + 0.001*eat?\" + 0.001*it + 0.001*frogs + 0.001*dishes, + 0.001*paid.',\n",
       " u'topic 13: 0.002*is + 0.001*that + 0.001*are + 0.001*somehow + 0.001*be + 0.001*again, + 0.001*it + 0.001*on + 0.001*typically + 0.001*they + 0.001*would + 0.001*wine-shops. + 0.001*cared + 0.001*she + 0.001*lack',\n",
       " u'topic 14: 0.002*is + 0.002*i + 0.001*on + 0.001*that + 0.001*tried + 0.001*one + 0.001*it + 0.001*were + 0.001*or + 0.001*they\"ve + 0.001*they + 0.001*are + 0.001*was + 0.001*as + 0.001*you',\n",
       " u'topic 15: 0.001*is + 0.001*you + 0.001*are + 0.001*that + 0.001*aristophanes + 0.001*it + 0.001*they + 0.001*tail. + 0.001*be + 0.001*rocket + 0.001*saigon + 0.001*indians + 0.001*bundles + 0.001*name + 0.001*broadcast',\n",
       " u'topic 16: 0.002*is + 0.001*that + 0.001*was + 0.001*wiped + 0.001*it + 0.001*have + 0.001*preserve + 0.001*from + 0.001*you + 0.001*are + 0.001*5. + 0.001*carp + 0.001*homes. + 0.001*list. + 0.001*on',\n",
       " u'topic 17: 0.002*is + 0.002*that + 0.001*he + 0.001*you + 0.001*then, + 0.001*are + 0.001*there + 0.001*into + 0.001*track + 0.001*it + 0.001*with + 0.001*up-to-date. + 0.001*was + 0.001*on + 0.001*vote',\n",
       " u'topic 18: 0.001*is + 0.001*are + 0.001*you + 0.001*that + 0.001*it + 0.001*from + 0.001*its + 0.001*when + 0.001*i + 0.001*on + 0.001*chiefly + 0.001*was + 0.001*he + 0.001*sports + 0.001*rocks.',\n",
       " u'topic 19: 0.002*is + 0.001*it + 0.001*have + 0.001*was + 0.001*that + 0.001*are + 0.001*they + 0.001*on + 0.001*\"look + 0.001*towns. + 0.001*\"we + 0.001*across. + 0.001*he + 0.001*be + 0.001*campaigns',\n",
       " u'topic 20: 0.002*is + 0.002*are + 0.002*that + 0.001*you + 0.001*or + 0.001*they + 0.001*not + 0.001*with + 0.001*be + 0.001*it + 0.001*so + 0.001*bulb + 0.001*was + 0.001*from + 0.001*fully',\n",
       " u'topic 21: 0.002*is + 0.001*that + 0.001*as + 0.001*are + 0.001*it + 0.001*but + 0.001*you + 0.001*great + 0.001*sight + 0.001*jimmy + 0.001*be + 0.001*on + 0.001*lemon + 0.001*deny + 0.001*ear.',\n",
       " u'topic 22: 0.001*is + 0.001*cancel + 0.001*was + 0.001*are + 0.001*you + 0.001*on + 0.001*that + 0.001*it + 0.001*at + 0.001*be + 0.001*what + 0.001*or + 0.001*will + 0.001*as + 0.001*i',\n",
       " u'topic 23: 0.002*is + 0.001*on + 0.001*they + 0.001*that + 0.001*it + 0.001*have + 0.001*i + 0.001*forest + 0.001*are + 0.001*you + 0.001*were + 0.001*as + 0.001*noon + 0.001*long. + 0.001*females',\n",
       " u'topic 24: 0.002*is + 0.001*it + 0.001*that + 0.001*earth + 0.001*on + 0.001*settled. + 0.001*well. + 0.001*you + 0.001*magnesium + 0.001*are + 0.001*was + 0.001*we\"ve + 0.001*first + 0.001*window. + 0.001*be',\n",
       " u'topic 25: 0.002*is + 0.001*as + 0.001*it + 0.001*he + 0.001*that + 0.001*you + 0.001*have + 0.001*are + 0.001*say. + 0.001*be + 0.001*they + 0.001*on + 0.001*courageous + 0.001*interests + 0.001*girls.',\n",
       " u'topic 26: 0.002*is + 0.001*you + 0.001*from + 0.001*that + 0.001*not + 0.001*are + 0.001*would + 0.001*can + 0.001*or + 0.001*was + 0.001*as + 0.001*it + 0.001*they + 0.001*this + 0.001*i',\n",
       " u'topic 27: 0.002*is + 0.002*are + 0.002*it + 0.001*i + 0.001*that + 0.001*by + 0.001*you + 0.001*when + 0.001*was + 0.001*herself + 0.001*they + 0.001*hospital + 0.001*he + 0.001*on + 0.001*effectively',\n",
       " u'topic 28: 0.002*is + 0.001*that + 0.001*it + 0.001*you + 0.001*by + 0.001*on + 0.001*only + 0.001*he + 0.001*this + 0.001*or + 0.001*at + 0.001*from + 0.001*pollution. + 0.001*was + 0.001*are',\n",
       " u'topic 29: 0.002*is + 0.001*that + 0.001*you + 0.001*disliked + 0.001*drawn + 0.001*us? + 0.001*it + 0.001*are + 0.001*represented + 0.001*was + 0.001*one\"s + 0.001*this + 0.001*up + 0.001*be + 0.001*he',\n",
       " u'topic 30: 0.002*is + 0.001*virginia. + 0.001*remarks + 0.001*all + 0.001*that + 0.001*are + 0.001*be + 0.001*it + 0.001*they + 0.001*language. + 0.001*but + 0.001*you + 0.001*mother\"s + 0.001*lived + 0.001*was',\n",
       " u'topic 31: 0.001*is + 0.001*was + 0.001*he + 0.001*are + 0.001*that + 0.001*it + 0.001*brain + 0.001*safe + 0.001*lake. + 0.001*price. + 0.001*(24 + 0.001*famous + 0.001*height + 0.001*frederick + 0.001*fall',\n",
       " u'topic 32: 0.002*is + 0.001*are + 0.001*build. + 0.001*that + 0.001*doesn\"t + 0.001*bird,\" + 0.001*fluids + 0.001*adds + 0.001*be + 0.001*jed. + 0.001*it + 0.001*miles + 0.001*was + 0.001*minh + 0.001*you',\n",
       " u'topic 33: 0.002*is + 0.001*around + 0.001*outrun + 0.001*resource. + 0.001*on + 0.001*excitement + 0.001*are + 0.001*grows + 0.001*that + 0.001*weeks + 0.001*tucked + 0.001*supermarket. + 0.001*more + 0.001*was + 0.001*it',\n",
       " u'topic 34: 0.002*is + 0.001*it + 0.001*that + 0.001*are + 0.001*on + 0.001*be + 0.001*had + 0.001*was + 0.001*or + 0.001*at + 0.001*jumped + 0.001*beginning, + 0.001*you + 0.001*detective + 0.001*about',\n",
       " u'topic 35: 0.002*is + 0.001*you + 0.001*at + 0.001*that + 0.001*now + 0.001*are + 0.001*they + 0.001*blanket + 0.001*this + 0.001*by + 0.001*it + 0.001*nano3 + 0.001*be + 0.001*have + 0.001*on',\n",
       " u'topic 36: 0.002*is + 0.001*that + 0.001*he + 0.001*was + 0.001*on + 0.001*it + 0.001*trait. + 0.001*are + 0.001*cylinder. + 0.001*thing. + 0.001*delta + 0.001*i + 0.001*agustin + 0.001*laying + 0.001*semicircular',\n",
       " u'topic 37: 0.002*is + 0.001*it + 0.001*was + 0.001*i + 0.001*what + 0.001*create + 0.001*fanny, + 0.001*points + 0.001*that + 0.001*rulers + 0.001*lakes + 0.001*this + 0.001*not + 0.001*fleet + 0.001*as',\n",
       " u'topic 38: 0.002*is + 0.002*it + 0.001*that + 0.001*be + 0.001*on + 0.001*have + 0.001*from + 0.001*you + 0.001*i + 0.001*sand + 0.001*this + 0.001*as + 0.001*if + 0.001*defined. + 0.001*they',\n",
       " u'topic 39: 0.002*is + 0.001*that + 0.001*at + 0.001*i + 0.001*he + 0.001*his + 0.001*it + 0.001*are + 0.001*\"but, + 0.001*wants + 0.001*you + 0.001*phenomenon, + 0.001*they\"ll + 0.001*drew + 0.001*was',\n",
       " u'topic 40: 0.002*is + 0.001*it + 0.001*are + 0.001*that + 0.001*they + 0.001*you + 0.001*have + 0.001*on + 0.001*don\"t + 0.001*i + 0.001*names + 0.001*durable + 0.001*transportation. + 0.001*he + 0.001*angle',\n",
       " u'topic 41: 0.002*is + 0.001*have + 0.001*he + 0.001*that + 0.001*are + 0.001*on + 0.001*locks + 0.001*it + 0.001*cord + 0.001*you + 0.001*i + 0.001*world\"s + 0.001*from + 0.001*possibly + 0.001*anger,',\n",
       " u'topic 42: 0.002*is + 0.001*you + 0.001*pulls + 0.001*that + 0.001*it + 0.001*are + 0.001*globe. + 0.001*was + 0.001*society + 0.001*outrun + 0.001*have + 0.001*slaves + 0.001*destroy + 0.001*bubble + 0.001*they',\n",
       " u'topic 43: 0.001*is + 0.001*matter + 0.001*or + 0.001*was + 0.001*result + 0.001*are + 0.001*there + 0.001*make + 0.001*he + 0.001*independence + 0.001*they + 0.001*it + 0.001*size. + 0.001*multiplied + 0.001*into',\n",
       " u'topic 44: 0.002*is + 0.001*on + 0.001*that + 0.001*are + 0.001*it\"s + 0.001*you + 0.001*was + 0.001*harden + 0.001*i + 0.001*sour + 0.001*it + 0.001*guitar + 0.001*have + 0.001*kind, + 0.001*trousers',\n",
       " u'topic 45: 0.002*is + 0.001*it + 0.001*that + 0.001*are + 0.001*they + 0.001*you + 0.001*fuels + 0.001*was + 0.001*together. + 0.001*whence + 0.001*personal + 0.001*there + 0.001*as + 0.001*movements + 0.001*ph.',\n",
       " u'topic 46: 0.001*is + 0.001*that + 0.001*knowledge, + 0.001*policemen + 0.001*long + 0.001*it + 0.001*machine + 0.001*faster + 0.001*are + 0.001*selected + 0.001*sold. + 0.001*ph. + 0.001*visible + 0.001*was + 0.001*continents,',\n",
       " u'topic 47: 0.001*is + 0.001*are + 0.001*result + 0.001*as + 0.001*it + 0.001*that + 0.001*they + 0.001*with + 0.001*he + 0.001*chippy\"s + 0.001*minh + 0.001*disappeared + 0.001*you + 0.001*suffered + 0.001*was',\n",
       " u'topic 48: 0.002*is + 0.001*are + 0.001*your + 0.001*product + 0.001*trying + 0.001*fossil. + 0.001*that + 0.001*it + 0.001*wearing + 0.001*on + 0.001*from + 0.001*interested + 0.001*tropical + 0.001*equivalent + 0.001*boxes',\n",
       " u'topic 49: 0.002*is + 0.001*breath. + 0.001*are + 0.001*that + 0.001*place + 0.001*carefully + 0.001*laughed. + 0.001*streets, + 0.001*it + 0.001*cold.\" + 0.001*ptolemy\"s + 0.001*sang, + 0.001*be + 0.001*i + 0.001*score',\n",
       " u'topic 50: 0.002*is + 0.001*pounds. + 0.001*on + 0.001*that + 0.001*are + 0.001*it + 0.001*they + 0.001*board + 0.001*you + 0.001*parakeet + 0.001*involved + 0.001*storehouses + 0.001*at + 0.001*fuzzy + 0.001*was',\n",
       " u'topic 51: 0.001*is + 0.001*she + 0.001*are + 0.001*it + 0.001*you + 0.001*that + 0.001*within + 0.001*cured + 0.001*1800s, + 0.001*madden, + 0.001*controlled + 0.001*strength. + 0.001*sugar + 0.001*editorial + 0.001*ruined',\n",
       " u'topic 52: 0.001*is + 0.001*nest. + 0.001*that + 0.001*are + 0.001*devices + 0.001*on + 0.001*tree + 0.001*dig + 0.001*coats. + 0.001*hard, + 0.001*huge + 0.001*have + 0.001*it + 0.001*that...\" + 0.001*was',\n",
       " u'topic 53: 0.002*is + 0.002*as + 0.001*it + 0.001*that + 0.001*about + 0.001*are + 0.001*you + 0.001*by + 0.001*was + 0.001*he + 0.001*they + 0.001*have + 0.001*has + 0.001*on + 0.001*neutral.',\n",
       " u'topic 54: 0.001*is + 0.001*was + 0.001*it + 0.001*are + 0.001*that + 0.001*this + 0.001*mountains. + 0.001*states, + 0.001*thirty + 0.001*islands + 0.001*they + 0.001*higher. + 0.001*flat, + 0.001*clubs + 0.001*named',\n",
       " u'topic 55: 0.002*is + 0.001*are + 0.001*it + 0.001*that + 0.001*howard + 0.001*most + 0.001*turned + 0.001*mortgage + 0.001*on + 0.001*they + 0.001*mercury. + 0.001*nerves + 0.001*subject. + 0.001*past. + 0.001*was',\n",
       " u'topic 56: 0.002*is + 0.001*it + 0.001*year. + 0.001*sophomore + 0.001*that + 0.001*his + 0.001*help, + 0.001*ball, + 0.001*are + 0.001*horses, + 0.001*i + 0.001*school.\" + 0.001*\"i + 0.001*have + 0.001*you',\n",
       " u'topic 57: 0.001*is + 0.001*pastures. + 0.001*are + 0.001*hand. + 0.001*tea + 0.001*that + 0.001*it + 0.001*nashville + 0.001*like?\" + 0.001*station + 0.001*cells, + 0.001*ago + 0.001*\"it\"s + 0.001*lungs, + 0.001*book,',\n",
       " u'topic 58: 0.002*is + 0.001*that + 0.001*from + 0.001*railroads, + 0.001*are + 0.001*be + 0.001*it + 0.001*you + 0.001*have + 0.001*style + 0.001*income + 0.001*smoke + 0.001*lycopod + 0.001*scratch + 0.001*oviduct',\n",
       " u'topic 59: 0.001*is + 0.001*they + 0.001*morning. + 0.001*smallest + 0.001*on + 0.001*allows + 0.001*that + 0.001*are + 0.001*can + 0.001*moisture + 0.001*it + 0.001*same, + 0.001*at + 0.001*was + 0.001*from',\n",
       " u'topic 60: 0.002*is + 0.001*that + 0.001*are + 0.001*it + 0.001*they + 0.001*city\"s + 0.001*you + 0.001*from + 0.001*was + 0.001*floods + 0.001*food, + 0.001*on + 0.001*as + 0.001*(about + 0.001*ways',\n",
       " u'topic 61: 0.001*is + 0.001*that + 0.001*dining + 0.001*hopped, + 0.001*be + 0.001*are + 0.001*tiger\"s + 0.001*administer + 0.001*was + 0.001*it + 0.001*earthquake, + 0.001*printer + 0.001*thompson + 0.001*his + 0.001*give',\n",
       " u'topic 62: 0.002*is + 0.001*it + 0.001*many + 0.001*lawmaking + 0.001*are + 0.001*fungi + 0.001*radio + 0.001*clanking + 0.001*friction + 0.001*fished + 0.001*agustin. + 0.001*you + 0.001*possession + 0.001*work, + 0.001*on',\n",
       " u'topic 63: 0.002*is + 0.001*that + 0.001*are + 0.001*home + 0.001*it + 0.001*searching + 0.001*by + 0.001*you + 0.001*finds + 0.001*be + 0.001*they + 0.001*silly + 0.001*have + 0.001*was + 0.001*\"now,',\n",
       " u'topic 64: 0.001*are + 0.001*is + 0.001*at + 0.001*bread. + 0.001*that + 0.001*was + 0.001*ford + 0.001*you + 0.001*fishing + 0.001*stable + 0.001*from + 0.001*ability + 0.001*aristophanes\" + 0.001*dropped + 0.001*independence,',\n",
       " u'topic 65: 0.001*is + 0.001*that + 0.001*many + 0.001*on + 0.001*they + 0.001*was + 0.001*you + 0.001*wagon. + 0.001*are + 0.001*it + 0.001*an + 0.001*may + 0.001*furniture. + 0.001*born + 0.001*college',\n",
       " u'topic 66: 0.002*is + 0.001*are + 0.001*sayre, + 0.001*that + 0.001*it + 0.001*patrol + 0.001*responses + 0.001*there + 0.001*i + 0.001*mercury. + 0.001*on + 0.001*shore, + 0.001*cast + 0.001*people + 0.001*counter.',\n",
       " u'topic 67: 0.001*is + 0.001*have + 0.001*was + 0.001*it + 0.001*muffled + 0.001*regions. + 0.001*stanley + 0.001*that + 0.001*this + 0.001*colony + 0.001*seventeenth + 0.001*cancer, + 0.001*patients + 0.001*they + 0.001*on',\n",
       " u'topic 68: 0.001*writing, + 0.001*smile + 0.001*homework. + 0.001*inside + 0.001*build. + 0.001*are + 0.001*ideas + 0.001*was + 0.001*presented + 0.001*any. + 0.001*be + 0.001*shift + 0.001*charlie + 0.001*tiptoed + 0.001*conflicts',\n",
       " u'topic 69: 0.001*is + 0.001*that + 0.001*lets + 0.001*you + 0.001*have + 0.001*was + 0.001*on + 0.001*are + 0.001*radiation + 0.001*this + 0.001*she + 0.001*ride + 0.001*piano + 0.001*it + 0.001*hide.',\n",
       " u'topic 70: 0.002*is + 0.001*much + 0.001*erosion. + 0.001*be + 0.001*community + 0.001*you + 0.001*are + 0.001*brothers + 0.001*as + 0.001*that + 0.001*igneous + 0.001*frontier + 0.001*was + 0.001*they + 0.001*one',\n",
       " u'topic 71: 0.001*is + 0.001*that + 0.001*also, + 0.001*premarital + 0.001*loam + 0.001*was + 0.001*blowing + 0.001*americans + 0.001*on + 0.001*as + 0.001*it + 0.001*are + 0.001*pollution + 0.001*or + 0.001*riches.',\n",
       " u'topic 72: 0.001*opportunities + 0.001*is + 0.001*are + 0.001*promises + 0.001*time,\" + 0.001*mountain. + 0.001*rule. + 0.001*water, + 0.001*worn + 0.001*mammals + 0.001*latitude. + 0.001*800 + 0.001*year\"s + 0.001*opportunity + 0.001*that',\n",
       " u'topic 73: 0.001*is + 0.001*game, + 0.001*on + 0.001*you + 0.001*sock. + 0.001*popular + 0.001*she + 0.001*charge + 0.001*eggs + 0.001*it + 0.001*they + 0.001*were + 0.001*begins, + 0.001*final + 0.001*chapter',\n",
       " u'topic 74: 0.002*is + 0.001*that + 0.001*on + 0.001*it + 0.001*thompson + 0.001*inland + 0.001*slope. + 0.001*like + 0.001*vomiting, + 0.001*was + 0.001*explained. + 0.001*you + 0.001*are + 0.001*be + 0.001*douglas',\n",
       " u'topic 75: 0.002*is + 0.001*from + 0.001*properly + 0.001*that + 0.001*you + 0.001*are + 0.001*giant\"s + 0.001*nice + 0.001*cuba, + 0.001*america, + 0.001*maybe + 0.001*fires + 0.001*oils. + 0.001*was + 0.001*make',\n",
       " u'topic 76: 0.002*is + 0.001*with + 0.001*that + 0.001*he + 0.001*it + 0.001*on + 0.001*was + 0.001*biological + 0.001*or + 0.001*what + 0.001*are + 0.001*i + 0.001*newspaper, + 0.001*collective + 0.001*you',\n",
       " u'topic 77: 0.002*is + 0.001*it + 0.001*counting + 0.001*on + 0.001*that + 0.001*way + 0.001*violet + 0.001*all + 0.001*they + 0.001*are + 0.001*was + 0.001*lieutenant + 0.001*laboratories. + 0.001*ready. + 0.001*\"two',\n",
       " u'topic 78: 0.002*is + 0.001*seasons + 0.001*that + 0.001*it + 0.001*needle + 0.001*are + 0.001*wrong, + 0.001*keogh + 0.001*at + 0.001*you + 0.001*everything. + 0.001*planning. + 0.001*tissue + 0.001*object. + 0.001*nearly',\n",
       " u'topic 79: 0.001*is + 0.001*or + 0.001*you + 0.001*that + 0.001*not + 0.001*as + 0.001*salts + 0.001*transmit + 0.001*doors + 0.001*i + 0.001*it + 0.001*on + 0.001*cheerful. + 0.001*are + 0.001*territories',\n",
       " u'topic 80: 0.001*is + 0.001*are + 0.001*that + 0.001*have + 0.001*she + 0.001*como. + 0.001*polls + 0.001*lever. + 0.001*sunlight. + 0.001*you + 0.001*run. + 0.001*these + 0.001*it + 0.001*learning + 0.001*airport.',\n",
       " u'topic 81: 0.001*is + 0.001*that + 0.001*graders + 0.001*was + 0.001*from + 0.001*month + 0.001*you + 0.001*they + 0.001*goddesses + 0.001*fall, + 0.001*are + 0.001*it + 0.001*on + 0.001*bathtub + 0.001*sofa',\n",
       " u'topic 82: 0.001*is + 0.001*feelings + 0.001*assemblies + 0.001*ragweed + 0.001*have + 0.001*year, + 0.001*crying + 0.001*you + 0.001*c.[calhoun + 0.001*suitable + 0.001*are + 0.001*that + 0.001*it + 0.001*read, + 0.001*from',\n",
       " u'topic 83: 0.002*is + 0.001*cream + 0.001*or + 0.001*that + 0.001*are + 0.001*emotions + 0.001*it + 0.001*linnaeus. + 0.001*milky + 0.001*you + 0.001*careful,\" + 0.001*away,\" + 0.001*makes + 0.001*found + 0.001*cigarettes',\n",
       " u'topic 84: 0.002*is + 0.001*as + 0.001*are + 0.001*by + 0.001*that + 0.001*it + 0.001*on + 0.001*or + 0.001*you + 0.001*was + 0.001*forests, + 0.001*has + 0.001*be + 0.001*ohio. + 0.001*when',\n",
       " u'topic 85: 0.002*is + 0.001*it + 0.001*that + 0.001*expose + 0.001*contrast + 0.001*press + 0.001*you + 0.001*sir + 0.001*are + 0.001*particles. + 0.001*debtor\"s + 0.001*hall + 0.001*didn\"t + 0.001*spreading + 0.001*maine,',\n",
       " u'topic 86: 0.002*is + 0.001*zealand, + 0.001*it + 0.001*that + 0.001*gentille. + 0.001*1, + 0.001*hill + 0.001*are + 0.001*you + 0.001*fuel + 0.001*debbie + 0.001*pursue + 0.001*grain. + 0.001*they + 0.001*be',\n",
       " u'topic 87: 0.001*that + 0.001*is + 0.001*i + 0.001*it + 0.001*trapped + 0.001*late.\" + 0.001*on + 0.001*had + 0.001*fails + 0.001*are + 0.001*car + 0.001*now, + 0.001*house?\" + 0.001*let\"s + 0.001*was',\n",
       " u'topic 88: 0.001*is + 0.001*be + 0.001*but + 0.001*are + 0.001*that + 0.001*all + 0.001*it + 0.001*tail + 0.001*was + 0.001*leave + 0.001*french + 0.001*cannot + 0.001*behavior, + 0.001*limit + 0.001*wagner',\n",
       " u'topic 89: 0.001*is + 0.001*his + 0.001*are + 0.001*with + 0.001*sponge + 0.001*effects + 0.001*be + 0.001*that + 0.001*ability + 0.001*at + 0.001*it + 0.001*you + 0.001*have + 0.001*i + 0.001*skin',\n",
       " u'topic 90: 0.002*is + 0.001*from + 0.001*are + 0.001*it + 0.001*that + 0.001*was + 0.001*95 + 0.001*think, + 0.001*have + 0.001*lap. + 0.001*hills, + 0.001*sayings + 0.001*operates + 0.001*horse. + 0.001*tasks,',\n",
       " u'topic 91: 0.001*group + 0.001*separated + 0.001*that + 0.001*is + 0.001*rubber. + 0.001*was + 0.001*reindeer + 0.001*write? + 0.001*on + 0.001*as + 0.001*alongside + 0.001*fellow + 0.001*obtain + 0.001*it + 0.001*gran,\"',\n",
       " u'topic 92: 0.001*is + 0.001*have + 0.001*from + 0.001*that + 0.001*center + 0.001*you + 0.001*it + 0.001*harnessed + 0.001*as + 0.001*sunken + 0.001*about, + 0.001*has + 0.001*clay. + 0.001*swim! + 0.001*are',\n",
       " u'topic 93: 0.002*is + 0.001*that + 0.001*at + 0.001*can + 0.001*preserved + 0.001*are + 0.001*it + 0.001*flows. + 0.001*english. + 0.001*i + 0.001*usually + 0.001*little + 0.001*lycopod + 0.001*you + 0.001*tube.',\n",
       " u'topic 94: 0.001*is + 0.001*opened. + 0.001*that + 0.001*it + 0.001*with + 0.001*why + 0.001*massachusetts + 0.001*be + 0.001*at + 0.001*about + 0.001*you + 0.001*or + 0.001*coin. + 0.001*\"i + 0.001*grouping',\n",
       " u'topic 95: 0.001*is + 0.001*are + 0.001*export + 0.001*limits. + 0.001*on + 0.001*pony, + 0.001*that + 0.001*he + 0.001*paint, + 0.001*clouds, + 0.001*was + 0.001*it + 0.001*mothers + 0.001*thinks + 0.001*employee',\n",
       " u'topic 96: 0.002*is + 0.001*that + 0.001*shown + 0.001*you + 0.001*allows + 0.001*on + 0.001*your + 0.001*sand. + 0.001*soon, + 0.001*ploughing + 0.001*it + 0.001*union, + 0.001*are + 0.001*have + 0.001*his',\n",
       " u'topic 97: 0.001*is + 0.001*that + 0.001*was + 0.001*civil + 0.001*energy + 0.001*naka\"s + 0.001*it + 0.001*you + 0.001*cheeks + 0.001*i + 0.001*are + 0.001*as + 0.001*with + 0.001*finding + 0.001*needs,',\n",
       " u'topic 98: 0.002*is + 0.001*you + 0.001*that + 0.001*skins + 0.001*on + 0.001*are + 0.001*make + 0.001*he + 0.001*into + 0.001*but + 0.001*edison + 0.001*as + 0.001*weekly + 0.001*choices, + 0.001*perfectly',\n",
       " u'topic 99: 0.001*is + 0.001*it + 0.001*racing + 0.001*are + 0.001*mountains. + 0.001*on + 0.001*that + 0.001*nerves + 0.001*hole + 0.001*philosophers + 0.001*\"they + 0.001*renewable + 0.001*today. + 0.001*confusion + 0.001*was',\n",
       " u'topic 100: 0.002*is + 0.001*that + 0.001*as + 0.001*i + 0.001*you + 0.001*it + 0.001*far + 0.001*those + 0.001*silicon + 0.001*scintillation + 0.001*play, + 0.001*land, + 0.001*covered + 0.001*territories. + 0.001*was',\n",
       " u'topic 101: 0.001*is + 0.001*planning, + 0.001*it + 0.001*ruth + 0.001*coronary + 0.001*they + 0.001*was + 0.001*that + 0.001*pores. + 0.001*decaying + 0.001*volts + 0.001*tools,\" + 0.001*books + 0.001*grew + 0.001*are',\n",
       " u'topic 102: 0.002*that + 0.001*is + 0.001*with + 0.001*are + 0.001*smoke + 0.001*was + 0.001*you + 0.001*she + 0.001*no. + 0.001*suburb + 0.001*it + 0.001*editor + 0.001*always + 0.001*beyond + 0.001*hill;',\n",
       " u'topic 103: 0.002*is + 0.001*that + 0.001*with + 0.001*from + 0.001*was + 0.001*you + 0.001*it + 0.001*or + 0.001*on + 0.001*miami + 0.001*she + 0.001*he + 0.001*one + 0.001*wolf + 0.001*liters',\n",
       " u'topic 104: 0.001*is + 0.001*was + 0.001*that + 0.001*are + 0.001*at + 0.001*it + 0.001*aboard. + 0.001*heard + 0.001*rights, + 0.001*lip + 0.001*effect + 0.001*bear + 0.001*author + 0.001*like + 0.001*17.',\n",
       " u'topic 105: 0.001*is + 0.001*that + 0.001*you + 0.001*i + 0.001*bedouin + 0.001*knits. + 0.001*cohn. + 0.001*on + 0.001*at + 0.001*wealth. + 0.001*it + 0.001*pipeline + 0.001*aviation + 0.001*scientists. + 0.001*can',\n",
       " u'topic 106: 0.002*is + 0.001*pick + 0.001*are + 0.001*land. + 0.001*they + 0.001*fashion. + 0.001*they\"d + 0.001*that + 0.001*(a + 0.001*had + 0.001*payment + 0.001*seed + 0.001*sperm + 0.001*dinner, + 0.001*underneath',\n",
       " u'topic 107: 0.001*is + 0.001*classification + 0.001*that + 0.001*role. + 0.001*green + 0.001*it + 0.001*he + 0.001*shortest + 0.001*normal. + 0.001*you + 0.001*together + 0.001*himself. + 0.001*have + 0.001*youth, + 0.001*on.',\n",
       " u'topic 108: 0.002*is + 0.001*that + 0.001*just + 0.001*you + 0.001*me + 0.001*was + 0.001*squeezed + 0.001*he + 0.001*romania + 0.001*are + 0.001*outside + 0.001*it + 0.001*they + 0.001*air + 0.001*apical',\n",
       " u'topic 109: 0.001*is + 0.001*are + 0.001*andes + 0.001*as + 0.001*contract + 0.001*that + 0.001*has + 0.001*stimulants + 0.001*but + 0.001*broccoli + 0.001*it + 0.001*screaming + 0.001*trade + 0.001*or + 0.001*occur.',\n",
       " u'topic 110: 0.002*is + 0.001*bull + 0.001*mom + 0.001*that + 0.001*breath + 0.001*light + 0.001*help + 0.001*you... + 0.001*bedrock. + 0.001*are + 0.001*shore. + 0.001*it + 0.001*people? + 0.001*labeled + 0.001*waiter',\n",
       " u'topic 111: 0.002*are + 0.001*is + 0.001*young. + 0.001*were + 0.001*that + 0.001*all + 0.001*they + 0.001*by, + 0.001*compared + 0.001*element + 0.001*noon. + 0.001*you + 0.001*facing + 0.001*from + 0.001*\"as',\n",
       " u'topic 112: 0.002*is + 0.001*enemy, + 0.001*not + 0.001*it + 0.001*commandos + 0.001*his + 0.001*was + 0.001*that + 0.001*you + 0.001*end + 0.001*are + 0.001*genie + 0.001*have + 0.001*respected + 0.001*damaged.',\n",
       " u'topic 113: 0.002*is + 0.001*engines. + 0.001*you + 0.001*that + 0.001*silt, + 0.001*it + 0.001*drought + 0.001*are + 0.001*half + 0.001*was + 0.001*about + 0.001*affairs + 0.001*reports. + 0.001*practical + 0.001*motor',\n",
       " u'topic 114: 0.001*is + 0.001*that + 0.001*it + 0.001*on + 0.001*i + 0.001*meals + 0.001*from + 0.001*are + 0.001*you + 0.001*they + 0.001*was + 0.001*candles + 0.001*calendar. + 0.001*action. + 0.001*headline',\n",
       " u'topic 115: 0.001*is + 0.001*it + 0.001*writing, + 0.001*hardened + 0.001*file + 0.001*invented. + 0.001*that + 0.001*frederick + 0.001*growls + 0.001*day. + 0.001*you + 0.001*hands. + 0.001*\"that + 0.001*congress + 0.001*rock',\n",
       " u'topic 116: 0.002*is + 0.001*it + 0.001*that + 0.001*moving + 0.001*noticed + 0.001*slope. + 0.001*mom\"s + 0.001*inland + 0.001*past + 0.001*she + 0.001*they + 0.001*are + 0.001*great. + 0.001*on + 0.001*or',\n",
       " u'topic 117: 0.001*is + 0.001*that + 0.001*had, + 0.001*they + 0.001*i + 0.001*are + 0.001*it + 0.001*produces + 0.001*pots + 0.001*jawless + 0.001*was + 0.001*reaction, + 0.001*prison + 0.001*as + 0.001*doubt',\n",
       " u'topic 118: 0.002*is + 0.001*that + 0.001*are + 0.001*their + 0.001*on + 0.001*came, + 0.001*picked + 0.001*was + 0.001*same, + 0.001*or + 0.001*attacks. + 0.001*time, + 0.001*fool + 0.001*people + 0.001*middle',\n",
       " u'topic 119: 0.001*sunken + 0.001*asks. + 0.001*are + 0.001*commando + 0.001*you + 0.001*not + 0.001*related. + 0.001*was + 0.001*fortunately, + 0.001*angry. + 0.001*bird.\" + 0.001*that + 0.001*he + 0.001*hearing, + 0.001*clothing.',\n",
       " u'topic 120: 0.002*is + 0.001*that + 0.001*or + 0.001*have + 0.001*poisoning + 0.001*calendar + 0.001*are + 0.001*when + 0.001*middle, + 0.001*so + 0.001*gametophytes + 0.001*it + 0.001*bedouin + 0.001*blackboard. + 0.001*groups',\n",
       " u'topic 121: 0.001*is + 0.001*snakes + 0.001*it + 0.001*century + 0.001*are + 0.001*practically + 0.001*spending + 0.001*were + 0.001*he + 0.001*recognition + 0.001*supported + 0.001*that + 0.001*bought + 0.001*bathing + 0.001*foreign',\n",
       " u'topic 122: 0.001*is + 0.001*sick. + 0.001*building + 0.001*that + 0.001*it + 0.001*back + 0.001*are + 0.001*dinosaurs + 0.001*victory + 0.001*you + 0.001*they + 0.001*accurate + 0.001*know,\" + 0.001*cannot + 0.001*on',\n",
       " u'topic 123: 0.001*have + 0.001*are + 0.001*then. + 0.001*mountains. + 0.001*is, + 0.001*training + 0.001*watermelon, + 0.001*flashed + 0.001*although + 0.001*pulled + 0.001*that + 0.001*good?\" + 0.001*mix + 0.001*money.\" + 0.001*gravity.',\n",
       " u'topic 124: 0.002*is + 0.001*are + 0.001*that + 0.001*it + 0.001*he + 0.001*11-3. + 0.001*with + 0.001*person. + 0.001*lycopod + 0.001*earth + 0.001*on + 0.001*herself. + 0.001*they + 0.001*you + 0.001*there',\n",
       " u'topic 125: 0.001*is + 0.001*are + 0.001*i + 0.001*price + 0.001*that + 0.001*be + 0.001*as + 0.001*but + 0.001*contact, + 0.001*calendar + 0.001*ear. + 0.001*underlying + 0.001*grounds + 0.001*at + 0.001*on',\n",
       " u'topic 126: 0.001*is + 0.001*border + 0.001*are + 0.001*nothing + 0.001*that + 0.001*from + 0.001*taken + 0.001*it + 0.001*studied. + 0.001*robinson + 0.001*you + 0.001*on + 0.001*well + 0.001*nail. + 0.001*yet',\n",
       " u'topic 127: 0.001*is + 0.001*that + 0.001*it + 0.001*waters + 0.001*degrees + 0.001*too.\" + 0.001*today,\" + 0.001*clothing, + 0.001*loosened + 0.001*sandstone. + 0.001*response + 0.001*counting + 0.001*you + 0.001*i + 0.001*they',\n",
       " u'topic 128: 0.002*is + 0.001*i + 0.001*was + 0.001*that + 0.001*problem.\" + 0.001*it + 0.001*travels + 0.001*will + 0.001*nuclear + 0.001*are + 0.001*you + 0.001*do,\" + 0.001*they + 0.001*as + 0.001*unexpected',\n",
       " u'topic 129: 0.001*is + 0.001*alone. + 0.001*go, + 0.001*area + 0.001*are + 0.001*personal + 0.001*this + 0.001*identical + 0.001*that + 0.001*his + 0.001*before. + 0.001*thinking, + 0.001*it + 0.001*register + 0.001*forth',\n",
       " u'topic 130: 0.001*is + 0.001*inferior + 0.001*fingerlike + 0.001*are + 0.001*that + 0.001*often + 0.001*practiced + 0.001*feature + 0.001*scheme + 0.001*cord + 0.001*doctors + 0.001*defeated + 0.001*broadcast + 0.001*chosen + 0.001*practical',\n",
       " u'topic 131: 0.002*is + 0.001*that + 0.001*life + 0.001*stars, + 0.001*you + 0.001*jones,\" + 0.001*it + 0.001*textbook + 0.001*all + 0.001*parade. + 0.001*reached + 0.001*faults + 0.001*energy + 0.001*are + 0.001*bought.',\n",
       " u'topic 132: 0.002*is + 0.001*meters. + 0.001*can + 0.001*it + 0.001*be + 0.001*said. + 0.001*was + 0.001*geiger + 0.001*that + 0.001*limit + 0.001*extended + 0.001*you + 0.001*into + 0.001*what + 0.001*are',\n",
       " u'topic 133: 0.001*is + 0.001*that + 0.001*elections + 0.001*growth, + 0.001*are + 0.001*refineries + 0.001*cases, + 0.001*was + 0.001*they + 0.001*sri + 0.001*\"they + 0.001*colonists + 0.001*sprayed + 0.001*it + 0.001*stack',\n",
       " u'topic 134: 0.001*is + 0.001*are + 0.001*be + 0.001*invaded + 0.001*that + 0.001*water + 0.001*you + 0.001*on + 0.001*their + 0.001*pennies. + 0.001*it + 0.001*ziemer + 0.001*were + 0.001*as + 0.001*expressed',\n",
       " u'topic 135: 0.001*is + 0.001*but + 0.001*settlement + 0.001*battle + 0.001*are + 0.001*that + 0.001*reaction. + 0.001*i + 0.001*it + 0.001*from + 0.001*seller + 0.001*alive. + 0.001*found + 0.001*pupa. + 0.001*barns.',\n",
       " u'topic 136: 0.001*is + 0.001*burn. + 0.001*all + 0.001*it + 0.001*lowlands + 0.001*that + 0.001*wished + 0.001*lightning + 0.001*\"good.\" + 0.001*empty + 0.001*from + 0.001*on + 0.001*about + 0.001*are + 0.001*polls',\n",
       " u'topic 137: 0.001*is + 0.001*have + 0.001*word. + 0.001*are + 0.001*that + 0.001*valuable + 0.001*editorial + 0.001*kitchen. + 0.001*was + 0.001*it + 0.001*aboard + 0.001*cook. + 0.001*wages + 0.001*they + 0.001*lack',\n",
       " u'topic 138: 0.001*it + 0.001*is + 0.001*streets, + 0.001*screws, + 0.001*that + 0.001*crowded + 0.001*hill; + 0.001*\"i\"ll + 0.001*determined + 0.001*be + 0.001*trap + 0.001*low, + 0.001*knew + 0.001*are + 0.001*was',\n",
       " u'topic 139: 0.001*is + 0.001*that + 0.001*are + 0.001*substances + 0.001*about. + 0.001*sheet + 0.001*it + 0.001*i + 0.001*decrease + 0.001*prairies. + 0.001*was + 0.001*began, + 0.001*this. + 0.001*interested + 0.001*history',\n",
       " u'topic 140: 0.001*is + 0.001*westward + 0.001*are + 0.001*have + 0.001*it + 0.001*you + 0.001*that + 0.001*rarely + 0.001*some + 0.001*upstairs + 0.001*his + 0.001*sir.\" + 0.001*meet + 0.001*mount + 0.001*superior.',\n",
       " u'topic 141: 0.001*is + 0.001*that + 0.001*control, + 0.001*they + 0.001*wave + 0.001*are + 0.001*neutral + 0.001*sit + 0.001*card?\" + 0.001*have + 0.001*there + 0.001*dishonored + 0.001*it + 0.001*strip + 0.001*biting',\n",
       " u'topic 142: 0.001*by + 0.001*is + 0.001*stepped + 0.001*scholars + 0.001*that + 0.001*broken + 0.001*are + 0.001*where + 0.001*mill + 0.001*plain + 0.001*replies + 0.001*history + 0.001*on + 0.001*hotel + 0.001*italy.',\n",
       " u'topic 143: 0.001*is + 0.001*one + 0.001*are + 0.001*that + 0.001*spending + 0.001*tall, + 0.001*pearl + 0.001*communicate + 0.001*forecasters + 0.001*planes + 0.001*speed + 0.001*realistic + 0.001*it + 0.001*many + 0.001*sheds',\n",
       " u'topic 144: 0.002*is + 0.001*on + 0.001*it + 0.001*are + 0.001*or + 0.001*that + 0.001*not + 0.001*dashing + 0.001*arabia + 0.001*these + 0.001*6 + 0.001*weeks, + 0.001*they + 0.001*couples + 0.001*he',\n",
       " u'topic 145: 0.001*is + 0.001*wake + 0.001*on + 0.001*that + 0.001*you + 0.001*serious + 0.001*are + 0.001*rope. + 0.001*were + 0.001*month + 0.001*light + 0.001*have. + 0.001*wave + 0.001*conduct + 0.001*sponges',\n",
       " u'topic 146: 0.001*is + 0.001*stimulants + 0.001*are + 0.001*that + 0.001*will, + 0.001*elements. + 0.001*you + 0.001*only + 0.001*movements + 0.001*it + 0.001*replaced + 0.001*roads + 0.001*dixiecrat + 0.001*pressures. + 0.001*lander',\n",
       " u'topic 147: 0.002*is + 0.001*samples + 0.001*which + 0.001*that + 0.001*fact, + 0.001*squirrels + 0.001*beach + 0.001*eventually + 0.001*marshall, + 0.001*was + 0.001*1978, + 0.001*other + 0.001*one-half + 0.001*expected + 0.001*sisters.',\n",
       " u'topic 148: 0.001*is + 0.001*it + 0.001*living + 0.001*that + 0.001*i.d.\"s + 0.001*yellowstone + 0.001*young. + 0.001*you + 0.001*partly + 0.001*they + 0.001*have + 0.001*his + 0.001*sit + 0.001*room, + 0.001*kinds.',\n",
       " u'topic 149: 0.001*is + 0.001*grass. + 0.001*pasture. + 0.001*root + 0.001*very + 0.001*he + 0.001*said. + 0.001*offer + 0.001*has + 0.001*sailors + 0.001*altogether + 0.001*tube + 0.001*humanity + 0.001*base. + 0.001*anvil']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice how harmful punctuation can be.\n",
    "hdpmodel.print_topics(topics=-1,topn=15) # -1 prints all topics\n",
    "\n",
    "## These topics, to me, don't seem great -- probably because of the small corpus and negligent pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## At this point, there's lots you can do with the topics...\n",
    "## You can also serialize the fit model for later use:\n",
    "hdpmodel.save(wd + \"/tasa-20k.hdpmodel.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
