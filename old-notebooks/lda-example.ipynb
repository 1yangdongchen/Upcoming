{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I'm keeping this all in gensim, but scikit-learn, scipy+numpy, and nltk all \n",
    "##   have nice helper functions for some of this stuff...\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Set this to wherever you unpacked this example\n",
    "wd = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in a corpus. In this example, the file should be a line-delimeted set of documents.\n",
    "\n",
    "# There are better (in terms of RAM) ways to load the corpus such that is encoded as it's read.\n",
    "documents = []\n",
    "fin       = open(wd + \"/tasa.docs-line-delimited-20k\", \"rb\")\n",
    "for line in fin: documents.append(line.rstrip(\"\\n\"))\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Pre-processing: utterly important...can take a while (~1 minute on my desktop)\n",
    "\n",
    "# 1) load and remove stop-words\n",
    "stoplist = set()\n",
    "fin = open(wd + \"/stopwords.txt\",\"rb\") # A Gerow special\n",
    "for w in fin: stoplist.add(w.rstrip(\"\\n\"))\n",
    "fin.close()\n",
    "\n",
    "# Note the lowercasing...\n",
    "texts = [[word.lower() for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "# 2) Remove words that appear only once:\n",
    "all_tokens  = sum(texts, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts       = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "## Other important things people tend to do here, in order of importance:\n",
    "##    1) Deal with punctuation.\n",
    "##    2) Remove words with a low and high document frequency.\n",
    "##    3) Remove words below a minimum average TF*IDF value.\n",
    "##    4) Stem words (NLTK has a couple good stemmers).\n",
    "##    5) Remove words not found in a dictionary (I have a great / huge dictionary; can take a while).\n",
    "##    6) Compute and include bigrams that have a collocational strength above some threshold (I use a top n sort of thing).\n",
    "##       scikit-learn has a great, but convoluted collocation extraction implementation.\n",
    "\n",
    "## The best rule of thumb I've heard for 2-4 is that the resulting dictionary should be between 15k and 25k words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create the dictionary (also called the vocabulary)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "## Convert our corpus to the gensim sparse representation. \n",
    "##This is the great part of the gensim implementations\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Don't need to run this for this example####\n",
    "\n",
    "## Getting to this point may take a while on large corpora. Luckily, you can serialize\n",
    "##   the corpus in the sparse representation for future use:\n",
    "\n",
    "corpora.MmCorpus.serialize(wd + '/tasa.mm', corpus) # store to disk, for later use\n",
    "dictionary.save(wd + '/tasa.dict')\n",
    "\n",
    "## And read it back in:\n",
    "corpus     = corpora.MmCorpus(wd + '/tasa.mm')\n",
    "dictionary = corpora.Dictionary.load(wd + '/tasa.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## And now we do some topic modeling. Here, I'm using Hierarchical Dirichlet Processe (HDP)\n",
    "##   which is just LDA that fits its own parameter for the number of topics. Gensim has\n",
    "##   a handful of other models, but HDP is one of the more advanced.\n",
    "## You may get some warnings for the first few iterations here.\n",
    "hdpmodel = models.HdpModel(corpus, id2word=dictionary)\n",
    "\n",
    "# Print the topics (a bit ugly, but you get the picture...)\n",
    "# Also notice how harmful punctuation can be.\n",
    "hdpmodel.print_topics(topics=-1,topn=15) # -1 prints all topics\n",
    "\n",
    "## These topics, to me, don't seem great -- probably because of the small corpus and negligent pre-processing.\n",
    "\n",
    "## At this point, there's lots you can do with the topics...\n",
    "## You can also serialize the fit model for later use:\n",
    "hdpmodel.save(wd + \"/tasa-20k.hdpmodel.gensim\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
