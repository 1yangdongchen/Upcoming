{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Semantic Networks - Network Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "\n",
    "import nltk #For POS tagging\n",
    "import sklearn #For generating some matrices\n",
    "import pandas as pd#For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #MAkes the plots look nice\n",
    "import IPython.display #For displaying images\n",
    "import networkx\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "#import subprocess\n",
    "import re\n",
    "\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pd.DataFrame(influence_matrix) \n",
    "    \n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Load the example data and tokenzie sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import networkx as nx\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDF = pd.read_csv('./data/reddit.csv', index_col = 0)\n",
    "redditDF = redditDF.sort_values('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get POS tagged_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-100:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores['normalized_sents'] = redditTopScores['sentences'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = None, stemmer = lucem_illud.stemmer_basic) for s in x])\n",
    "sents = redditTopScores['normalized_sents']\n",
    "tagged_sents = [nltk.pos_tag(sent[0]) for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'NN*|JJ*'  # you can specify your own pattern to extract name phrases\n",
    "noun_phrases = [[token for token, tag in tagged_sent if re.match(pattern, tag)] for tagged_sent in tagged_sents ]\n",
    "edgelist = [edge for phrase in noun_phrases for edge in it.combinations(phrase, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes(before filtering) in the graph is 348\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph(edgelist)\n",
    "print (\"Total number of nodes(before filtering) in the graph is %s\" % len(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need centrality scores to filter important words\n",
    "three centralities are applied:\n",
    "- betweenness centrality\n",
    "- closeness centrality\n",
    "- eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterWords(G,filter_ = \"betweenness\", rule = \"number\",value_of_rule = 200):\n",
    "    G = G.copy()\n",
    "    if filter_ ==\"betweenness\":\n",
    "        index = nx.betweenness_centrality(G) #betweeness centrality score\n",
    "    elif filter_ == \"closeness\":\n",
    "        index = nx.closeness_centrality(G) #closeness centrality score\n",
    "    elif filter_ == \"eigenvector\":\n",
    "        index = nx.eigenvector_centrality(G) #eigenvector centrality score\n",
    "    else:\n",
    "        print(\"wrong filter paremeter, should be: betweenness/closeness/eigenvector\")\n",
    "        raise\n",
    "    \n",
    "        \n",
    "    if rule=='number':# if filter by limiting the total number of nodes \n",
    "        \n",
    "        sorted_index = sorted(index.items(), key=lambda x:x[1], reverse=True)\n",
    "        value_of_rule = np.min([value_of_rule, len(G.nodes)])\n",
    "        \n",
    "        nodes_remain = {}\n",
    "        for word, centr in sorted_index[:value_of_rule]:\n",
    "            nodes_remain[word] = centr\n",
    "        G.remove_nodes_from([n for n in index if n not in nodes_remain])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G\n",
    "    \n",
    "    if rule=='value':# if filter by limiting the value of centrality\n",
    "        value_of_rule = np.max([float(value_of_rule),0])\n",
    "        G.remove_nodes_from([n for n in index if index[n] >=value_of_rule])\n",
    "        print (\"Total number of nodes(after filtering) in the graph is %s\" % len(G))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes(after filtering) in the graph is 200\n",
      "Total number of nodes(after filtering) in the graph is 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 343)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_1 = filterWords(G, filter_='betweenness',rule='number',value_of_rule=200) #filter by limiting the total # of nodes\n",
    "G_2 = filterWords(G, filter_='betweenness',rule='value',value_of_rule=0.05) # filter by limiting the value of centrality specified\n",
    "len(G_1),len(G_2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
