{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Semantic Networks - Bayesian Echo Chamber\n",
    "\n",
    "This week, we explore the representation and analysis of semantic networks. A word or document network is an unsupervized representation of text akin to a clustering or an embedding, but semantic networks can also be defined using semantic or syntactic information derived from methods we have used earlier in the quarter. For example, we can define links between words as a function of their co-presence within a document, chapter, paragraph, sentence, noun phrase or continuous bag of words. We can also define links as a function of words that rely on one another within a directed dependency parse, or links between extracted Subjects, Verbs and Objects, or nouns and the adjectives that modify them (or verbs and the adverbs that modify *them*). Rendering words linked as a network or discrete topology allows us to take advantage of the wide range of metrics and models developed for network analysis. These include measurement of network centrality, density and modularity, \"block modeling\" structurally equivalent relationships, andsophisticated graphical renderings of networks or network partitions that allow us to visually interrogate their structure and complexity.\n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "\n",
    "#This will be doing most of the work\n",
    "import networkx as nx\n",
    "\n",
    "import nltk #For POS tagging\n",
    "import sklearn #For generating some matrices\n",
    "import pandas as pd#For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #MAkes the plots look nice\n",
    "\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will primarily be dealing with graphs in this notebook, so lets first go over how to use them.\n",
    "\n",
    "To start with lets create an undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x118930dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nx.Graph()\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can add nodes. These are all named, like entries in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g.add_node(1)\n",
    "g.add_node(2)\n",
    "g.add_node(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have 3 vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or if we want to get more information about the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 3\n",
      "Number of edges: 0\n",
      "Average degree:   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can give nodes properties, like name or type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NN'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g.nodes[1]['type'] = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NN'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Still pretty boring though...\n",
    "\n",
    "Lets add a couple of edges. Notice that we use the ids not any of the properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4\n",
      "Number of edges: 4\n",
      "Average degree:   2.0000\n"
     ]
    }
   ],
   "source": [
    "g.add_edges_from([(1, 2), (2, 3), (3, 1), (1,4)])\n",
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note how the summary has changed. Also there's one more node, since we asked for an edge to 4,\n",
    "\n",
    "We can also give the edges properties like weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges[1, 2]['weight'] = 2\n",
    "g.edges[1, 4]['weight'] = 2\n",
    "g.edges[1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's visualize it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD3CAYAAAAXDE8fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH/X/AfTNOtzEfQqfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c84630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw_networkx(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Very exciting :-).\n",
    "\n",
    "There are a great many things to do with the graph once we have created it, some of which we will explore today.\n",
    "\n",
    "First lets load our data, the Grimmer corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>download_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>targetSenator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Apr2005Kennedy14.txt</td>\n",
       "      <td>raw/Kennedy/01Apr2005Kennedy14.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Aug2005Kennedy12.txt</td>\n",
       "      <td>raw/Kennedy/01Aug2005Kennedy12.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://raw.githubusercontent.com/lintool/Grim...</td>\n",
       "      <td>https://github.com/lintool/GrimmerSenatePressR...</td>\n",
       "      <td>01Aug2006Kennedy10.txt</td>\n",
       "      <td>raw/Kennedy/01Aug2006Kennedy10.txt</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        download_url  \\\n",
       "0  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "1  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "2  https://raw.githubusercontent.com/lintool/Grim...   \n",
       "\n",
       "                                            html_url                    name  \\\n",
       "0  https://github.com/lintool/GrimmerSenatePressR...  01Apr2005Kennedy14.txt   \n",
       "1  https://github.com/lintool/GrimmerSenatePressR...  01Aug2005Kennedy12.txt   \n",
       "2  https://github.com/lintool/GrimmerSenatePressR...  01Aug2006Kennedy10.txt   \n",
       "\n",
       "                                 path  \\\n",
       "0  raw/Kennedy/01Apr2005Kennedy14.txt   \n",
       "1  raw/Kennedy/01Aug2005Kennedy12.txt   \n",
       "2  raw/Kennedy/01Aug2006Kennedy10.txt   \n",
       "\n",
       "                                                text targetSenator  \n",
       "0           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...       Kennedy  \n",
       "1           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...       Kennedy  \n",
       "2           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...       Kennedy  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesDF = pandas.read_csv('../data/senReleasesTraining.csv', index_col = 0)\n",
    "senReleasesDF[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will be extracting sentences, as well as tokenizing and stemming. (You should be able to do this in your sleep now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For now we will not be dropping any stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = None, stemmer = lucem_illud.stemmer_basic) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start by looking at words co-occurring in the same sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wordCooccurrence(sentences, makeMatrix = False):\n",
    "    words = set()\n",
    "    for sent in sentences:\n",
    "        words |= set(sent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in sentences:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        g = nx.convert_matrix.from_numpy_matrix(coOcMat)\n",
    "        g = nx.relabel_nodes(g, {i : w for i, w in enumerate(wordLst)})\n",
    "        return g    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build a graph based on word cooccurence in the first 50 press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = wordCooccurrence(senReleasesDF['normalized_sents'][:50].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Total number of vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Total number of edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2473287"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A part of the adjacency matrix of cleaned word by press releases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[4., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 6., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.to_numpy_matrix(g)[:10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can save the graph and read it later, although this is slow if there are lots of edges or nodes, so we will filter first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#nx.write_graphml(g, '../data/Obama_words.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or, we can build graphs starting with a two-mode network. Let's again use the document-word frequency matrix that we used in week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenlist = nltk.word_tokenize(text)\n",
    "    normalized = lucem_illud.normalizeTokens(tokenlist, stopwordLst = lucem_illud.stop_words_basic, stemmer = lucem_illud.stemmer_basic)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "senVectorizer = sklearn.feature_extraction.text.CountVectorizer(tokenizer = tokenize)\n",
    "senVects_incidence = senVectorizer.fit_transform(senReleasesDF['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3233)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senVects_incidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to turn the incidence matrix into a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_2mode = nx.Graph()\n",
    "\n",
    "#define all the nodes\n",
    "g_2mode.add_nodes_from((senVectorizer.get_feature_names()[i] for i in range(senVects_incidence.shape[1])), type = 'word')\n",
    "g_2mode.add_nodes_from(range(senVects_incidence.shape[1]), type = 'doc')\n",
    "\n",
    "#add all the edges\n",
    "g_2mode.add_edges_from(((d, senVectorizer.get_feature_names()[w], {'weight' : senVects_incidence[d, w]}) for d, w in zip(*senVects_incidence.nonzero())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nx.info(g_2mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A very popular layout algorithm for visualizing graphs is the Fruchterman-Reingold Algorithm (or spring layout), which uses a physical metaphor for lay-out. Nodes repel one another, and edges draw connected elements together like springs. The algorithm attempts to minimize the energy in such a system. For a large graph, however, the algorithm is computational demanding. The commented code gives you a layout and you can save it as a pickle. Let's then load a layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#These take a long time\n",
    "#layout = nx.spring_layout(g_2mode)\n",
    "#pickle.dump(layout, open( \"layout.pkl\", \"wb\" ) )\n",
    "#layout = pickle.load(open('layout.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, let's plot the bipartite network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A faster algorithm for large networks is the Large Graph Layout algorithm. If we want even faster computation and tunable visualizations, check out [Pajek](http://mrvar.fdv.uni-lj.si/pajek/) or [gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layout = g.layout_lgl()\n",
    "ig.plot(g, layout = layout, vertex_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A two-mode network can be easily transformed into two one-mode network, enabling words to be connected to other words via the number of documents that share them, or documents to be connected to other documents via the words they share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gDoc, gWord = g.bipartite_projection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's first take a look at the document-to-document network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gDoc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's do a visualization. It is not surprising that almost every document is connected to every one else. We can use edge weight to distinguish distance (modeled as attraction) between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layout = gDoc.layout_fruchterman_reingold(weights = gDoc.es['weight'])\n",
    "ig.plot(gDoc, layout = layout, vertex_shape = 'circle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another way to visualize the graph is to binarize the adjacency matrix with some cutoff values for edge weight. Let's use the medianw weight as our cutoff threshhold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "median = np.median(gDoc.es['weight'])\n",
    "medEdges = gDoc.es.select(lambda x: x['weight'] > median)\n",
    "g1_d = gDoc.subgraph_edges(medEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g1_d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layout = g1_d.layout_fruchterman_reingold()\n",
    "ig.plot(g1_d, layout = layout, vertex_shape = 'circle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's turn it around and look at the word-to-word network (via documents). Instead of working directly with `gWord`, let's start with matrix multiplication which gives us more freedom to define edge weight. First, let's reduce the number of words to a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "senVectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=stop_words_nltk, tokenizer = tokenize, max_features = 200)\n",
    "senVects = senVectorizer.fit_transform(senReleasesDF['text'][:100])\n",
    "wordsLst = senVectorizer.get_feature_names()\n",
    "senVects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's define the weight of an edge to the be the number of document coocurrences of two words divided by the square root of the product of their marginal frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "A = senVects.todense() #Get the doc-word frequency matrix.\n",
    "M = np.sum(A, axis = 0) #The marginal frequencies are simplies the column sums.\n",
    "W = A.T.dot(A)/np.sqrt(M.T.dot(M)) #Get out weight matrix.\n",
    "np.fill_diagonal(W,0) #Set the diagonal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G = ig.Graph.Weighted_Adjacency(W.tolist(), mode=ig.ADJ_UNDIRECTED)\n",
    "G.vs['name'] = wordsLst #Names are what you reference\n",
    "G.vs['label'] = wordsLst #Label are what is displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layout = G.layout_fruchterman_reingold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ig.plot(G, layout = layout, vertex_size = 0, edge_width = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even better, we can input the weight matrix with some cutoff value. We use 4 for now as it is useful for demonstration, but you can explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cutoff = 4\n",
    "G_b = G.subgraph_edges(G.es.select(lambda x: x['weight'] > cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b = G.subgraph_edges(G.es.select(weight_gt = cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layout = G_b.layout_fruchterman_reingold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ig.plot(G_b, layout = layout, vertex_size = 0, edge_width = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can continue to trim globally or locally, to investigate the structure of words around a core word of interest.\n",
    "\n",
    "#Can we do an example of this...visualize just the neighborhood of a word...e.g., the word and all words most closely connected and their close connections (friend of a friend)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that render some reasonable networks to meaningfully characterize the structure of words and documents (or subdocuments like chapters or paragraphs) from your corpus. Also create some smaller, word neighborhood graphs. What are useful filters and thresholds and what semantic structures do they reveal that given insight into the social world and social game inscribed in your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Alternatively, there are some more informative networks statistics.\n",
    "\n",
    "Lets begin with measures of centrality. The concept of centrality is that some nodes (words or documents) are more *central* to the network than others. There are many distinct and opposing operationalizations of this concept, however. One important concept is *betweenness* centrality, which distinguishes nodes that require the most shortest pathways between all other nodes in the network. Semantically, words with a high betweenness centrality may link distinctive domains, rather than being \"central\" to any one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.vs[np.argmax(G_b.betweenness())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can color and size the nodes by betweenness centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pal = ig.GradientPalette(\"red\", \"blue\", G_b.vcount())\n",
    "G_b.vs['color'] = [pal.get(int(v)) for v in np.argsort(G_b.betweenness())]\n",
    "ig.plot(G_b, layout = layout, edge_width = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The distrubution of betweenness centrality is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(G_b.betweenness())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or if we set a max of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist([min(b, 500) for b in G_b.betweenness()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an exponential distrubution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another way to visualize the graph involes the use of label sizes to represent betweenness centrality and edge widths to represent edge weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge_width = (np.array(G_b.es['weight'])*0.1).tolist()\n",
    "size = (5 + np.sqrt(np.array(G_b.betweenness()))).tolist()\n",
    "ig.plot(G_b, layout = layout, vertex_size = 0, vertex_label_size = size, edge_width = edge_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here it appears that \"health\"/\"drug\", \"torture\"/\"Iraq\", and \"loan\"/\"lend\" are key concepts that connect others in the broader network. This is interesting in that they seem to be a domain-specific rather than linking words like \"require\" and \"govern\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What are the top ten words in terms of betweenness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.betweenness()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What are words further down (the lowest all have centralities of 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.betweenness()), key = lambda x: x[1], reverse = True)[140:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Alternatively, we can look at degree centrality, which is simply the number of connections possessed by each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.vs[np.argmax(G_b.degree())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pal = ig.GradientPalette(\"red\", \"blue\", max(G_b.degree()) + 1)\n",
    "G_b.vs['color'] = [pal.get(v) for v in G_b.degree()]\n",
    "ig.plot(G_b, layout = layout, edge_width = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The top 10 words by degree are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.degree()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And the bottom 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.degree()), key = lambda x: x[1], reverse = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The distrubtioon of degree looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(G_b.degree())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also look at closeness centrality, or the average Euclidean or path distance between a node and all others in the network. A node with the highest closeness centrality is most likely to send a signal with the most coverage to the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.vs[np.argmax(G_b.closeness())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pal = ig.GradientPalette(\"red\", \"blue\", G_b.vcount())\n",
    "G_b.vs['color'] = [pal.get(int(v)) for v in np.argsort(G_b.closeness())]\n",
    "ig.plot(G_b, layout = layout, edge_width = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.closeness()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.closeness()), key = lambda x: x[1], reverse = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or eignvector centrality, an approach that weights degree by the centrality of those to whom one is tied (and the degree to whom they are tied, etc.) In short, its an $n$th order degree measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.vs['color'] = [pal.get(int(v)) for v in np.argsort(G_b.eigenvector_centrality())]\n",
    "ig.plot(G_b, layout = layout, edge_width = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Top and bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.eigenvector_centrality()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(G_b.vs['name'], G_b.eigenvector_centrality()), key = lambda x: x[1], reverse = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate different kinds of centrality for distinct words or documents in a network composed from your corpus of interest. Which type of words tend to be most and least central? Can you identify how different centrality measures distinguish different kind of words in your corpus? What do these patterns suggest about the semantic content and structure of your documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also look at global statistics, like the density of a network, defined as the number of edges existing divided by the total number of edges possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also calculate the average degree per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.mean(G_b.degree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The diameter calculates the average distance between any two nodes in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.diameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Moreover, we can find cliques, or completely connected sets of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_b.largest_cliques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(', '.join((G_b.vs[i]['name'] for i in G_b.largest_cliques()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now lets look at a subgraph of the network, those nodes that are within 2 edges 'war'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "warNeighbors = G_b.neighbors('war')\n",
    "warNeighborsPlus1 = set(warNeighbors)\n",
    "for n in warNeighbors:\n",
    "    warNeighborsPlus1 |= set(G_b.neighbors(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_war = G_b.subgraph(G_b.vs.select(warNeighborsPlus1))\n",
    "G_war.vcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_war.vs['color'] = 'red'\n",
    "G_war.vs.find('war')['color'] = 'yellow'\n",
    "ig.plot(G_war, target = 'data/war_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There is a bug in iGraph that causes some plots to display incorrectly, but the svgs they produce are unaffected. So we can just display the svgs with Ipython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/war_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# POS based networks\n",
    "\n",
    "Now lets look at links between specific parts of speech within a network.\n",
    "\n",
    "For this we will be using the `nltk` POS facilities instead of the Stanford ones. These are much faster, but also somewhat less accurate. (You get what you *pay* for in computational power).\n",
    "\n",
    "Lets look at nouns co-occurring in sentences using the top 10 (by score) reddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('data/reddit.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-100:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we'll normalize the tokens through stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['normalized_sents'] = redditTopScores['sentences'].apply(lambda x: [normlizeTokens(s, stopwordLst = None, stemmer = snowball) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "redditTopScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def posCooccurrence(sentences, *posType, makeMatrix = False):\n",
    "    pal = ig.RainbowPalette(n = len(posType))\n",
    "    palMap = {p : pal.get(i) for i, p in enumerate(posType)}\n",
    "    words = set()\n",
    "    reducedSents = []\n",
    "    #Only using the first kind of POS for each word\n",
    "    wordsMap = {}\n",
    "    for sent in sentences:\n",
    "        s = [(w, t) for w, t in nltk.pos_tag(sent) if t in posType]\n",
    "        for w, t in s:\n",
    "            if w not in wordsMap:\n",
    "                wordsMap[w] = t\n",
    "        reducedSent = [w for w, t in s]\n",
    "        words |= set(reducedSent)\n",
    "        reducedSents.append(reducedSent)\n",
    "    wordLst = list(words)\n",
    "    wordIndices = {w: i for i, w in enumerate(wordLst)}\n",
    "    wordCoCounts = {}\n",
    "    #consider a sparse matrix if memory becomes an issue\n",
    "    coOcMat = np.zeros((len(wordIndices), len(wordIndices)))\n",
    "    for sent in reducedSents:\n",
    "        for i, word1 in enumerate(sent):\n",
    "            word1Index = wordIndices[word1]\n",
    "            for word2 in sent[i + 1:]:\n",
    "                coOcMat[word1Index][wordIndices[word2]] += 1\n",
    "    if makeMatrix:\n",
    "        return coOcMat, wordLst\n",
    "    else:\n",
    "        coOcMat = coOcMat.T + coOcMat\n",
    "        edges = list(zip(*np.where(coOcMat)))\n",
    "        weights = coOcMat[np.where(coOcMat)]\n",
    "        kinds = [wordsMap[w] for w in wordLst]\n",
    "        colours = [palMap[k] for k in kinds]\n",
    "        g = ig.Graph( n = len(wordLst),\n",
    "            edges = edges,\n",
    "            vertex_attrs = {'name' : wordLst, \n",
    "                            'label' : wordLst, \n",
    "                            'kind' : kinds,\n",
    "                            'color' : colours,\n",
    "                           },\n",
    "            edge_attrs = {'weight' : weights})\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN.vcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a bit to large to effectively visilize, so let's remove the verices whose degree is less than or equal to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN_d = gNN.subgraph(gNN.vs.select(lambda x: x.degree() > 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN_d.vcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ig.plot(gNN_d, target = 'data/gNN_d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/gNN_d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is still a hairball because we retained the most highly connected veritices. Let's trim a few edges as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN_d.ecount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN_d = gNN_d.subgraph_edges(gNN_d.es.select(lambda x: x['weight'] > 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNN_d.ecount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I know what the graph is like and the default layout is fine\n",
    "ig.plot(gNN_d, target ='data/gNN_d2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/gNN_d2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That is an interesting pattern, everyone is talking about themselves (\"I...this\", \"I...that\"). <span style=\"color:red\">**How do you interpret all those self loops?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for e in gNN_d.es.select(lambda x: x.is_loop()):\n",
    "    print(gNN_d.vs[e.source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What if we want to look at noun-verb pairs instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV = posCooccurrence(redditTopScores['normalized_sents'].sum(), 'NN', 'VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`gNV` has co-occurrences between nouns and nouns as well as between verbs and verbs. Let's remove these and make it purely about noun and verb combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV.ecount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_p = gNV.subgraph_edges(gNV.es.select(lambda x: gNV.vs[x.source]['kind'] != gNV.vs[x.target]['kind']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_p.ecount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Dropping low weight edges and low degree vertices again gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_p = gNV_p.subgraph(gNV_p.vs.select(lambda x: x.degree() > 50))\n",
    "gNV_p = gNV_p.subgraph_edges(gNV_p.es.select(lambda x: x['weight'] > 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_p.vcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layoutNV = gNV_p.layout_fruchterman_reingold()\n",
    "ig.plot(gNV_p, layout = layoutNV, target = 'data/gNV_p.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/gNV_p.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at this plot it looks like `I` and `be` are the centers of two communities. Lets check this by partitioning the networks, a graphical method of clustering. Our first approach will use the information theoretic infomap algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(gNV_p.community_infomap(edge_weights = gNV_p.es['weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's focus on one of these communities and create an \"ego network\" surrounding a single (important) word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_I = gNV.subgraph(gNV.vs.find('i').neighbors())\n",
    "#Still need to filter it a bit to display\n",
    "gNV_I.vcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_I = gNV_I.subgraph(gNV_I.vs.select(lambda x: x.degree() > 10))\n",
    "gNV_I = gNV_I.subgraph_edges(gNV_I.es.select(lambda x: x['weight'] > 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layoutNV_I = gNV_I.layout_fruchterman_reingold(repulserad = .001)\n",
    "ig.plot(gNV_I, layout = layoutNV_I, target = 'data/gNV_I.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/gNV_I.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead of just those connect to a vertex we can find all those connected to it within 2 hops, lets look at 'stori' for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV.vs.find('stori')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "storyNeighbors = gNV.neighbors('stori')\n",
    "storyNeighborsPlus1 = set(storyNeighbors)\n",
    "for n in storyNeighbors:\n",
    "    storyNeighborsPlus1 |= set(gNV.neighbors(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_story = gNV.subgraph(gNV.vs.select(storyNeighborsPlus1))\n",
    "gNV_story.vcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A large network, but we can compute some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(gNV_story.vs['name'], gNV_story.degree()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or by eignvector centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(gNV_story.vs['name'], gNV_story.eigenvector_centrality()), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that 'stori' isn't even in the top 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets filter it a bit than plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gNV_story = gNV_story.subgraph(gNV_story.vs.select(lambda x: x.degree() > 5 or x['name'] == 'stori'))\n",
    "gNV_story = gNV_story.subgraph_edges(gNV_story.es.select(lambda x: x['weight'] > 10))\n",
    "gNV_story.vs.find('stori')['color'] = 'yellow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layoutNV_story = gNV_story.layout_fruchterman_reingold(repulserad = .001)\n",
    "ig.plot(gNV_story, layout = layoutNV_story, target = 'data/gNV_story.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('data/gNV_story.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I is still in the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that construct at least two different networks comprising different combinations of word types, linked by different syntactic structures, that illuminate your corpus and the dynamics you are interested to explore. Graph these networks or subnetworks within them. What are relationships that are meaningful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Echo Chamber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fakeEnglish(length):\n",
    "    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n",
    "    return ''.join(np.random.choice(listd,length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your own dataset should contains 4 columns(with the same column names)  as the faked one below\n",
    "- name: name of the participant\n",
    "- tokens: a list of tokens in one utterance\n",
    "- start: starting time of utterance (unit doesn't matter, can be 'seconds','minute','hour'...)\n",
    "- end: ending time of utterance\n",
    "\n",
    "\n",
    "No need to sort data for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script= []\n",
    "language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n",
    "role = 'Adult' #parameter, no need to tune \n",
    "\n",
    "for i in range(290):\n",
    "    dt = []\n",
    "    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n",
    "    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n",
    "    dt.append(faketokens) #fake utterance\n",
    "    dt.append(i*2+np.random.random()) # start time\n",
    "    dt.append(i*2+1+np.random.random()) # end time\n",
    "    script.append(dt)\n",
    "\n",
    "df_transcript = pd.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n",
    "df_transcript.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_TalkbankXML(df,fname,database='my_dataset',language='eng'):\n",
    "    '''\n",
    "    This function will transform pandas dataframe into TalkbankXML for Bayesian estimation of influence.\n",
    "    \n",
    "    df:  pandas dataframe of utterences, need four columns: \"name\", \"tokens\", \"start\", \"end\".  \n",
    "    (\"start\", \"end\" is the start_time, and end_time of a utterence)\n",
    "    \n",
    "    fname:  file name to save xml output\n",
    "    database: (arbitrary) name of your database \n",
    "    language: 'eng' is the default. (currently also support 'chinese')\n",
    "    \n",
    "    Important: All non-English tokens and names should be unicode. \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #format dataframe\n",
    "    df = df[['name','tokens','start','end']]\n",
    "    unit='s'\n",
    "    df['start'] = df['start'].apply(pd.to_numeric)\n",
    "    df['end'] = df['end'].apply(pd.to_numeric)\n",
    "    df = df.dropna() # only allow non-missing data\n",
    "    df = pd.DataFrame(sorted(df.values.tolist(),key=lambda x:x[2]),columns=df.columns) #sort by time\n",
    "    \n",
    "    #generate xml\n",
    "    fname_short = fname\n",
    "    if os.path.sep in fname:\n",
    "        new_path = \".\"+os.path.sep+'data'+os.path.sep+fname.split(os.path.sep)[-1].split('.')[0]+os.path.sep\n",
    "        fname_short = fname.split(os.path.sep)[-1].split(os.path.sep)[0]\n",
    "    else:\n",
    "        new_path = \".\"+os.path.sep+'data'+os.path.sep+fname.split('.')[0]+os.path.sep\n",
    "    if not '.' in fname_short:\n",
    "        fname_short+='.xml'\n",
    "    else:\n",
    "        fname_short = fname_short.split('.')[0]+'.xml'\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(new_path+fname_short,'w') as fw:\n",
    "        print ('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n',file=fw)\n",
    "        print ('<CHAT from=\"%s\">' % database,file=fw)\n",
    "        \n",
    "        #create person_ids\n",
    "        person_id ={}\n",
    "        for person in set(df_transcript['name'].values.tolist()):\n",
    "            if not person in person_id: person_id[person] = len(person_id)+1\n",
    "        \n",
    "        #add participants \n",
    "        print ('<Participants>',file=fw)\n",
    "        for person in person_id:\n",
    "            print ('<participant id=\"%s\" name=\"%s\" role=\"Adult\" language=\"%s\"/>' %(person,person,language),file=fw)\n",
    "        print ('</Participants>\\n',file=fw)   \n",
    "        \n",
    "        #add utterences\n",
    "        for row in df.values:\n",
    "            print ('<u who=\"%s\" uID=\"#%s\">' % (row[0],person_id[row[0]]),file=fw)\n",
    "            for word in row[1]:\n",
    "                print ('''<w>%s</w>''' % word, file=fw)\n",
    "               \n",
    "            print ('''<media start=\"%s\" end=\"%s\" unit=\"%s\"/>''' %(row[2],row[3],unit),file=fw)\n",
    "            print ('''</u>''',file=fw)\n",
    "            print ('''''', file=fw)\n",
    "        print (\"</CHAT>\\n\",file=fw)\n",
    "    \n",
    "    \n",
    "    print ('New File saved to %s' % new_path+fname_short)\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data into TalkbankXML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_fname = 'USpresident.xml'  #should be .xml\n",
    "language = 'eng' \n",
    "#language = 'chinese'\n",
    "make_TalkbankXML(df_transcript,output_fname,language =language )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Echo Chamber to get estimation.\n",
    "- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n",
    "- Larger \"Vocab_size\" (see below) will cost more time\n",
    "- Larger \"sampling_time\" will also consume more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n",
    "sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n",
    "subprocess.call([\"python\", \"./src/run_bec.py\",output_fname,str(Vocab_size),language,str(int(sampling_time))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtime output can be viewed in shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_fname = '12-angry-men.xml'   #uncomment this line if want to use the example dataset: \"12-angry-men\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_path = './results/'+output_fname.split('.')[0]+os.path.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_meta_info = pd.read_table(result_path+ 'meta-info.txt',header=None)\n",
    "df_log_prob = pd.read_csv(result_path+\"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n",
    "df_influence = pd.read_csv(result_path+ 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n",
    "#print(df_log_prob.head())\n",
    "df_log_prob.shape,df_influence.shape\n",
    "df_participants = pd.read_csv(result_path+ 'cast.txt',delim_whitespace=True) #\n",
    "df_participants.head(2)\n",
    "person_id = pd.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n",
    "print()\n",
    "print ('Person : ID')\n",
    "person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDensity(df):\n",
    "    data = df#_log_prob['log.prior']\n",
    "    density = gaussian_kde(data)\n",
    "    width = np.max(data)-np.min(data)\n",
    "    xs = np.linspace(np.min(data)-width/5,np.max(data)+width/5,600)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    return xs,density(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MCMC trace and density of log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.plot(df_log_prob['log.prior'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Trace of log.prior')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "x,y = getDensity(df_log_prob['log.prior'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prior')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.plot(df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.likelihood')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "x,y = getDensity(df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.plot(df_log_prob['log.likelihood.test.set'])\n",
    "plt.title('Trace of log.likelihood.test.set')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood.test.set')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.prob')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Influence Matrix between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = int(np.sqrt(len(df_influence.columns))) #number of participants\n",
    "id_person = {}\n",
    "for p in person_id:\n",
    "    id_person[person_id[p]]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pd.DataFrame(influence_matrix) \n",
    "    \n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked = df_influence.mean(axis=0)\n",
    "df_mean = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.std(axis=0)\n",
    "df_std = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('MEAN of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('SD of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barplot of total influences sent/received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sender_std = {} #sd of total influence sent\n",
    "reciever_std = {} #sd of total influence recieved\n",
    "for i in range(A):\n",
    "    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n",
    "    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n",
    "\n",
    "sent = df_mean.sum(axis=1) #mean of total influence sent\n",
    "recieved =df_mean.sum(axis=0) #mean of total influence recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"\\t\\tTotal linguistic influence sent/received \")\n",
    "ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n",
    "\n",
    "plt.grid()\n",
    "wd=0.45\n",
    "ii=0\n",
    "for p in sender_std:\n",
    "    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    ii+=1\n",
    "ii=0\n",
    "for p in reciever_std:\n",
    "    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    ii+=1\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n",
    "plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('speaker',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Influence Network!\n",
    "- you can visualize any of the influence matrices above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch, Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawNetwork(df,title):\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    G = nx.DiGraph()\n",
    "    for from_ in df.index:\n",
    "        for to_ in df.columns:\n",
    "            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n",
    "            \n",
    "    pos = nx.spring_layout(G,k=0.55,iterations=20)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    weights = np.array(weights)\n",
    "    #weights = weights*weights\n",
    "    weights = 6*weights/np.max(weights)\n",
    "    print(title)\n",
    "    \n",
    "    edge_colors=20*(weights/np.max(weights))\n",
    "    edge_colors = edge_colors.astype(int)\n",
    "#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
    "#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
    "#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
    "    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n",
    "            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n",
    "            alpha=0.75,arrows=True,arrowsize=20)\n",
    "    return edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get quantile influence matrices for 25%, 50%, 75% quantile\n",
    "stacked = df_influence.quantile(0.25)\n",
    "df_q25 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.5)\n",
    "df_q50 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.75)\n",
    "df_q75 = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_mean = drawNetwork(df_mean,'Mean Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
